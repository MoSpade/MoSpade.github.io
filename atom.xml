<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://MoSpade.github.io/</id>
    <title>MoSpade</title>
    <updated>2021-10-25T11:13:59.974Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://MoSpade.github.io/"/>
    <link rel="self" href="https://MoSpade.github.io/atom.xml"/>
    <subtitle>我没见过,但也许存在</subtitle>
    <logo>https://MoSpade.github.io/images/avatar.png</logo>
    <icon>https://MoSpade.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, MoSpade</rights>
    <entry>
        <title type="html"><![CDATA[ mysql中date_add()函数的使用?]]></title>
        <id>https://MoSpade.github.io/post/mysql-zhong-date_addhan-shu-de-shi-yong/</id>
        <link href="https://MoSpade.github.io/post/mysql-zhong-date_addhan-shu-de-shi-yong/">
        </link>
        <updated>2021-10-15T08:13:35.000Z</updated>
        <content type="html"><![CDATA[<p><strong>需求描述</strong>:</p>
<p>在使用mysql的过程中,需要对日期进行计算,比如对某个日期加上几天,几个小时等操作,</p>
<p>在此记录下,date_add()函数的使用.</p>
<p><strong>操作过程</strong>:</p>
<p>date_add()函数语法:</p>
<pre><code>　　DATE_ADD(date,INTERVAL expr unit)
</code></pre>
<p>备注:<strong>date_add()和adddate()是同义词</strong>.</p>
<p><strong>参数说明</strong>:</p>
<p>date:起始日期或者起始时间</p>
<p>expr:指定的是一个间隔值,在起始时间中增加或者减少,<strong>注意</strong>:expr是一个字符串.对于负值间隔,可以以&quot;-&quot;开头</p>
<p>unit:表示的是一个单位,比如,加上的是1天还是一个小时.</p>
<p>1.对某个日期加上n天的操作</p>
<pre><code class="language-mysql">mysql&gt; select date_add('2018-06-26',INTERVAL '5' day);
+-----------------------------------------+
| date_add('2018-06-26',INTERVAL '5' day) |
+-----------------------------------------+
| 2018-07-01                              |
+-----------------------------------------+
1 row in set (0.00 sec)

mysql&gt; select date_add('2018-06-26',INTERVAL '-5' day);
+------------------------------------------+
| date_add('2018-06-26',INTERVAL '-5' day) |
+------------------------------------------+
| 2018-06-21                               |
+------------------------------------------+
1 row in set (0.01 sec)
</code></pre>
<p>备注:expr是字符串,如果加上的正值的天数,直接'5'即可,如果是减可以用date_sub函数或者expr为负值'-5'.</p>
<p>2.对某个日期加上n小时,n分钟,n秒的操作</p>
<pre><code>mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL 1 hour);  #对于日期加上1小时
+-------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL 1 hour) |
+-------------------------------------------------+
| 2018-06-27 00:59:59                             |
+-------------------------------------------------+
1 row in set (0.00 sec)

mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL 1 minute);  #对于日期加上1分钟
+---------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL 1 minute) |
+---------------------------------------------------+
| 2018-06-27 00:00:59                               |
+---------------------------------------------------+
1 row in set (0.00 sec)

mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL 1 second);  #对日期加上1秒钟
+---------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL 1 second) |
+---------------------------------------------------+
| 2018-06-27 00:00:00                               |
+---------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>​	3.对于某个日期加上n分钟n秒</p>
<pre><code>mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL '1:1' MINUTE_SECOND);
+--------------------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL '1:1' MINUTE_SECOND) |
+--------------------------------------------------------------+
| 2018-06-27 00:01:00                                          |
+--------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>备注:一次性加上1分钟,一秒.</p>
<p>4.对于某个日期加上n小时n分钟n秒</p>
<pre><code>mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL '1:1:1' HOUR_SECOND);
+--------------------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL '1:1:1' HOUR_SECOND) |
+--------------------------------------------------------------+
| 2018-06-27 01:01:00                                          |
+--------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>备注:单位HOUR_SECOND就是从小时到秒.expr:HOURS:MINUTES:SECONDS</p>
<p>5.对某个日期加上n小时n分钟</p>
<pre><code>mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL '1:1' HOUR_MINUTE);
+------------------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL '1:1' HOUR_MINUTE) |
+------------------------------------------------------------+
| 2018-06-27 01:00:59                                        |
+------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>备注:加上1小时1分钟</p>
<p>6.对某个日期加上几天几小时几分钟几秒钟</p>
<pre><code>mysql&gt; select date_add('2018-06-26 23:59:59',INTERVAL '2 2:1:1' DAY_SECOND);
+---------------------------------------------------------------+
| date_add('2018-06-26 23:59:59',INTERVAL '2 2:1:1' DAY_SECOND) |
+---------------------------------------------------------------+
| 2018-06-29 02:01:00                                           |
+---------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>备注:在日期上加上2天2小时2分钟1秒钟</p>
<p><strong>unit(单位)和expr(表达式)对照表</strong>:</p>
<figure data-type="image" tabindex="1"><img src="https://images2018.cnblogs.com/blog/1226424/201806/1226424-20180626135953354-1600572151.png" alt="img" loading="lazy"></figure>
<p><strong>小结</strong>:</p>
<p>对于某个日期的计算,无论加上多少,都可以根据这个表格进行调整.对于日期的减法,尽量用date_sub()函数来实现.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark core]]></title>
        <id>https://MoSpade.github.io/post/spark-core/</id>
        <link href="https://MoSpade.github.io/post/spark-core/">
        </link>
        <updated>2021-09-27T02:46:57.000Z</updated>
        <content type="html"><![CDATA[<p>尚硅谷大数据技术之SparkCore<br>
(作者：尚硅谷大数据研发部)<br>
版本：V3.0<br>
第1章 RDD概述<br>
1.1 什么是RDD<br>
RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。<br>
代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。<br>
1.1.1 RDD类比工厂生产</p>
<p>1.1.2 WordCount工作流程</p>
<p>1.2 RDD五大特性</p>
<p>第2章 RDD编程<br>
2.1 RDD的创建<br>
在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD、从外部存储创建RDD、从其他RDD创建。<br>
2.1.1 IDEA环境准备<br>
1）创建一个maven工程，工程名称叫SparkCoreTest</p>
<p>2）添加scala框架支持</p>
<p>3）创建一个scala文件夹，并把它修改为Source Root<br>
4）创建包名：com.atguigu.createrdd<br>
5）在pom文件中添加spark-core的依赖和scala的编译插件<br>
<dependencies><br>
<dependency><br>
<groupId>org.apache.spark</groupId><br>
<artifactId>spark-core_2.12</artifactId><br>
<version>3.0.0</version><br>
</dependency><br>
</dependencies></p>
<build>
    <finalName>SparkCoreTest</finalName>
    <plugins>
        <plugin>
            <groupId>net.alchim31.maven</groupId>
            <artifactId>scala-maven-plugin</artifactId>
            <version>3.4.6</version>
            <executions>
                <execution>
                    <goals>
                        <goal>compile</goal>
                        <goal>testCompile</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
2.1.2 从集合中创建
1）从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD
package com.atguigu.createrdd
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
<p>object createrdd01_array {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.使用parallelize()创建rdd
    val rdd: RDD[Int] = sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8))

    rdd.collect().foreach(println)

    //4.使用makeRDD()创建rdd
    val rdd1: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5, 6, 7, 8))

    rdd1.collect().foreach(println)

    sc.stop()
}
</code></pre>
<p>}<br>
注意：makeRDD有两种重构方法，重构方法一如下，makeRDD和parallelize功能一样。<br>
def makeRDD[T: ClassTag](<br>
seq: Seq[T],<br>
numSlices: Int = defaultParallelism): RDD[T] = withScope {<br>
parallelize(seq, numSlices)<br>
}<br>
2）makeRDD的重构方法二，增加了位置信息<br>
注意：只需要知道makeRDD不完全等于parallelize即可。<br>
def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {<br>
assertNotStopped()<br>
val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1.<em>2)).toMap<br>
new ParallelCollectionRDD[T](this, seq.map(</em>._1), math.max(seq.size, 1), indexToPrefs)<br>
}<br>
2.1.3 从外部存储系统的数据集创建<br>
由外部存储系统的数据集创建RDD包括：本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等。<br>
1）数据准备<br>
在新建的SparkCoreTest项目名称上右键=》新建input文件夹=》在input文件夹上右键=》分别新建1.txt和2.txt。每个文件里面准备一些word单词。<br>
2）创建RDD<br>
package com.atguigu.createrdd</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object createrdd02_file {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.读取文件。如果是集群路径：hdfs://hadoop102:8020/input
    val lineWordRdd: RDD[String] = sc.textFile(&quot;input&quot;)

    //4.打印
    lineWordRdd.foreach(println)

    //5.关闭
    sc.stop()
}
</code></pre>
<p>}<br>
2.1.4 从其他RDD创建<br>
主要是通过一个RDD运算完后，再产生新的RDD。<br>
详见2.3节<br>
2.1.5 创建IDEA快捷键<br>
1）点击File-&gt;Settings…-&gt;Editor-&gt;Live Templates-&gt;output-&gt;Live Template</p>
<p>2）点击左下角的Define-&gt;选择Scala</p>
<p>3）在Abbreviation中输入快捷键名称scc，在Template text中填写，输入快捷键后生成的内容。</p>
<p>//1.创建SparkConf并设置App名称<br>
val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)</p>
<p>//2.创建SparkContext，该对象是提交Spark App的入口<br>
val sc: SparkContext = new SparkContext(conf)</p>
<p>//4.关闭连接<br>
sc.stop()<br>
2.2 分区规则<br>
2.2.1 默认分区源码（RDD数据从集合中创建）<br>
1）默认分区数源码解读</p>
<p>2）创建一个包名：com.atguigu.partition<br>
3）代码验证<br>
package com.atguigu.partition</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object Partition01_Array_default {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    val rdd: RDD[Int] = sc.makeRDD(Array(1,2,3,4))

    //3. 输出数据，产生了12个分区
    rdd.saveAsTextFile(&quot;output&quot;)
     //结论:从集合创建rdd,如果不手动写分区数量的情况下,默认分区数跟本地模式的cpu核数有关
     //local : 1个   local[*] : 笔记本所有核心数    local[K]:K个

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
4）思考：数据就4个，分区却产生了12个，严重浪费资源，怎么办？<br>
2.2.2 分区源码（RDD数据从集合中创建）<br>
1）分区测试（RDD数据从集合中创建）<br>
object Partition02_Array {</p>
<pre><code>def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkCoreTest&quot;)
    val sc: SparkContext = new SparkContext(conf)

    //1）4个数据，设置4个分区，输出：0分区-&gt;1，1分区-&gt;2，2分区-&gt;3，3分区-&gt;4
    //val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 4)

    //2）4个数据，设置3个分区，输出：0分区-&gt;1，1分区-&gt;2，2分区-&gt;3,4
    //val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 3)

    //3）5个数据，设置3个分区，输出：0分区-&gt;1，1分区-&gt;2、3，2分区-&gt;4、5
    val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5), 3)

    rdd.saveAsTextFile(&quot;output&quot;)

    sc.stop()
}
</code></pre>
<p>}<br>
2）分区源码</p>
<p>分区的开始位置 = 分区号 * 数据总长度/分区总数<br>
分区的结束位置 =（分区号 + 1）* 数据总长度/分区总数<br>
2.2.3 默认分区源码（RDD数据从文件中读取后创建）<br>
1）分区测试<br>
object Partition03_file_default {</p>
<pre><code>def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkCoreTest&quot;)
    val sc: SparkContext = new SparkContext(conf)

    //1）默认分区的数量：默认取值为当前核数和2的最小值,一般为2
    //val rdd: RDD[String] = sc.textFile(&quot;input&quot;)

    rdd.saveAsTextFile(&quot;output&quot;)

    sc.stop()
}
</code></pre>
<p>}<br>
2）分区源码</p>
<p>2.2.4 分区源码（RDD数据从文件中读取后创建）<br>
1）分区测试<br>
object partition04_file {</p>
<pre><code>def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkCoreTest&quot;)
    val sc: SparkContext = new SparkContext(conf)

    //1）输入数据1-4，每行一个数字；输出：0=&gt;{1、2} 1=&gt;{3} 2=&gt;{4} 3=&gt;{空}
    //val rdd: RDD[String] = sc.textFile(&quot;input/3.txt&quot;, 3)

    rdd.saveAsTextFile(&quot;output&quot;)

    sc.stop()
}
</code></pre>
<p>}<br>
2）源码解析</p>
<p>注意：getSplits文件返回的是切片规划，真正读取是在compute方法中创建LineRecordReader读取的，有两个关键变量： start = split.getStart()	   end = start + split.getLength<br>
1.分区数量的计算方式:<br>
totalSize = 10<br>
goalSize = 10 / 3 = 3(byte) 表示每个分区存储3字节的数据<br>
分区数= totalSize/ goalSize = 10 /3 =&gt; 3,3,4<br>
4子节大于3子节的1.1倍,符合hadoop切片1.1倍的策略,因此会多创建一个分区,即一共有4个分区  3,3,3,1<br>
2. Spark读取文件，采用的是hadoop的方式读取，所以一行一行读取，跟字节数没有关系<br>
3.数据读取位置计算是以偏移量为单位来进行计算的。<br>
4.数据分区的偏移量范围的计算<br>
0 =&gt; [0,3]         1@@     012        0 =&gt; 1,2<br>
1 =&gt; [3,6]         2@@     345        1 =&gt; 3<br>
2 =&gt; [6,9]         3@@     678        2 =&gt; 4<br>
3 =&gt; [9,9]        4         9          3 =&gt; 无<br>
2.3 Transformation转换算子（面试开发重点）<br>
RDD整体上分为Value类型、双Value类型和Key-Value类型<br>
2.3.1 Value类型<br>
1）创建包名：com.atguigu.value<br>
2.3.1.1 map()映射</p>
<p>4）具体实现<br>
object value01_map {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(1 to 4, 2)

    // 3.2 调用map方法，每个元素乘以2
    val mapRdd: RDD[Int] = rdd.map(_ * 2)

    // 3.3 打印修改后的RDD中数据
    mapRdd.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.2 mapPartitions()以分区为单位执行Map</p>
<p>4）具体实现<br>
object value02_mapPartitions {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(1 to 4, 2)

    // 3.2 调用mapPartitions方法，每个元素乘以2
    val rdd1 = rdd.mapPartitions(x=&gt;x.map(_*2))

    // 3.3 打印修改后的RDD中数据
    rdd1.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.3 map()和mapPartitions()区别</p>
<p>2.3.1.4 mapPartitionsWithIndex()带分区号<br>
1）函数签名：<br>
def mapPartitionsWithIndex[U: ClassTag](<br>
f: (Int, Iterator[T]) =&gt; Iterator[U], // Int表示分区编号<br>
preservesPartitioning: Boolean = false): RDD[U]<br>
2）功能说明：类似于mapPartitions，比mapPartitions多一个整数参数表示分区号<br>
3）需求说明：创建一个RDD，使每个元素跟所在分区号形成一个元组，组成一个新的RDD</p>
<p>4）具体实现<br>
object value03_mapPartitionsWithIndex {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(1 to 4, 2)

    // 3.2 创建一个RDD，使每个元素跟所在分区号形成一个元组，组成一个新的RDD
    val indexRdd = rdd.mapPartitionsWithIndex( (index,items)=&gt;{items.map( (index,_) )} )

    // 3.3 打印修改后的RDD中数据
    indexRdd.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.5 flatMap()扁平化<br>
1）函数签名：def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]<br>
2）功能说明<br>
与map操作类似，将RDD中的每一个元素通过应用f函数依次转换为新的元素，并封装到RDD中。<br>
区别：在flatMap操作中，f函数的返回值是一个集合，并且会将每一个该集合中的元素拆分出来放到新的RDD中。<br>
3）需求说明：创建一个集合，集合里面存储的还是子集合，把所有子集合中数据取出放入到一个大的集合中。</p>
<p>4）具体实现：<br>
object value04_flatMap {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val listRDD=sc.makeRDD(List(List(1,2),List(3,4),List(5,6),List(7)), 2)

    // 3.2 把所有子集合中数据取出放入到一个大的集合中
    listRDD.flatMap(list=&gt;list).collect.foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.6 glom()分区转换数组<br>
1）函数签名：def glom(): RDD[Array[T]]<br>
2）功能说明<br>
该操作将RDD中每一个分区变成一个数组，并放置在新的RDD中，数组中元素的类型与原分区中元素类型一致<br>
3）需求说明：创建一个2个分区的RDD，并将每个分区的数据放到一个数组，求出每个分区的最大值</p>
<p>4）具体实现<br>
object value05_glom {</p>
<p>def main(args: Array[String]): Unit = {</p>
<pre><code>    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd = sc.makeRDD(1 to 4, 2)

    // 3.2 求出每个分区的最大值  0-&gt;1,2   1-&gt;3,4
    val maxRdd: RDD[Int] = rdd.glom().map(_.max)

    // 3.3 求出所有分区的最大值的和 2 + 4
    println(maxRdd.collect().sum)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.7 groupBy()分组</p>
<p>4）具体实现<br>
object value06_groupby {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd = sc.makeRDD(1 to 4, 2)

    // 3.2 将每个分区的数据放到一个数组并收集到Driver端打印
    rdd.groupBy(_ % 2).collect().foreach(println)

    // 3.3 创建一个RDD
    val rdd1: RDD[String] = sc.makeRDD(List(&quot;hello&quot;,&quot;hive&quot;,&quot;hadoop&quot;,&quot;spark&quot;,&quot;scala&quot;))

    // 3.4 按照首字母第一个单词相同分组
    rdd1.groupBy(str=&gt;str.substring(0,1)).collect().foreach(println)

    sc.stop()
}
</code></pre>
<p>}<br>
groupBy会存在shuffle过程<br>
shuffle：将不同的分区数据进行打乱重组的过程<br>
shuffle一定会落盘。可以在local模式下执行程序，通过4040看效果。<br>
2.3.1.8 GroupBy之WordCount</p>
<p>object value07_groupby_wordcount {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val strList: List[String] = List(&quot;Hello Scala&quot;, &quot;Hello Spark&quot;, &quot;Hello World&quot;)
    val rdd = sc.makeRDD(strList)

    // 3.2 将字符串拆分成一个一个的单词
    val wordRdd: RDD[String] = rdd.flatMap(str =&gt; str.split(&quot; &quot;))

    // 3.3 将单词结果进行转换：word=&gt;(word,1)
    val wordToOneRdd: RDD[(String, Int)] = wordRdd.map(word =&gt; (word, 1))

    // 3.4 将转换结构后的数据分组
    val groupRdd: RDD[(String, Iterable[(String, Int)])] = wordToOneRdd.groupBy(t =&gt; t._1)

    // 3.5 将分组后的数据进行结构的转换
    //        val wordToSum: RDD[(String, Int)] = groupRdd.map(
    //            t =&gt; (t._1, t._2.toList.size)
    //        )

    //        val wordToSum: RDD[(String, Int)] = groupRdd.map {
    //            x =&gt;
    //                x match {
    //                    case (word, list) =&gt; {
    //                        (word, list.size)
    //                    }
    //                }
    //        }

    val wordToSum: RDD[(String, Int)] = groupRdd.map {

        case (word, list) =&gt; {
            (word, list.size)
        }
    }

    // 3.6 打印输出
    wordToSum.collect().foreach(println)

    // 4 关闭资源
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.9 filter()过滤<br>
1）函数签名： def filter(f: T =&gt; Boolean): RDD[T]<br>
2）功能说明<br>
接收一个返回值为布尔类型的函数作为参数。当某个RDD调用filter方法时，会对该RDD中每一个元素应用f函数，如果返回值类型为true，则该元素会被添加到新的RDD中。<br>
3）需求说明：创建一个RDD，过滤出对2取余等于0的数据</p>
<p>4）代码实现<br>
object value08_filter {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 2)

    //3.1 过滤出符合条件的数据
    val filterRdd: RDD[Int] = rdd.filter(_ % 2 == 0)

    //3.2 收集并打印数据
    filterRdd.collect().foreach(println)

    //4 关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.10 sample()采样<br>
1）函数签名：<br>
def sample(<br>
withReplacement: Boolean,<br>
fraction: Double,<br>
seed: Long = Utils.random.nextLong): RDD[T]<br>
// withReplacement： true为有放回的抽样，false为无放回的抽样；<br>
// fraction表示：以指定的随机种子随机抽样出数量为fraction的数据；<br>
// seed表示：指定随机数生成器种子。<br>
2）功能说明<br>
从大量的数据中采样<br>
3）需求说明：创建一个RDD（1-10），从中选择放回和不放回抽样</p>
<p>4）代码实现<br>
object value09_sample {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.1 创建一个RDD
    val dataRDD: RDD[Int] = sc.makeRDD(List(1,2,3,4,5,6))

    // 抽取数据不放回（伯努利算法）
    // 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。
    // 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要
    // 第一个参数：抽取的数据是否放回，false：不放回
    // 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；
    // 第三个参数：随机数种子
    val sampleRDD: RDD[Int] = dataRDD.sample(false, 0.5)
    sampleRDD.collect().foreach(println)

    println(&quot;----------------------&quot;)

    // 抽取数据放回（泊松算法）
    // 第一个参数：抽取的数据是否放回，true：放回；false：不放回
    // 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数
    // 第三个参数：随机数种子
    val sampleRDD1: RDD[Int] = dataRDD.sample(true, 2)
    sampleRDD1.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
5）随机数测试<br>
public class TestRandom {</p>
<pre><code>public static void main(String[] args) {

    // 随机算法相同，种子相同，那么随机数就相同
    //Random r1 = new Random(100);
    // 不输入参数，种子取的当前时间的纳秒值，所以随机结果就不相同了
    Random r1 = new Random();

    for (int i = 0; i &lt; 5; i++) {

        System.out.println(r1.nextInt(10));
    }

    System.out.println(&quot;--------------&quot;);

    //Random r2 = new Random(100);
    Random r2 = new Random();

    for (int i = 0; i &lt; 5; i++) {

        System.out.println(r2.nextInt(10));
    }
}
</code></pre>
<p>}</p>
<h2 id="种子相同时的输出结果50481">种子相同时的输出结果：<br>
5<br>
0<br>
4<br>
8<br>
1</h2>
<p>5<br>
0<br>
4<br>
8<br>
1</p>
<p>2.3.1.11 distinct()去重</p>
<p>4）代码实现<br>
object value10_distinct {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val distinctRdd: RDD[Int] = sc.makeRDD(List(1,2,1,5,2,9,6,1))

    // 3.2 打印去重后生成的新RDD
    distinctRdd.distinct().collect().foreach(println)

    // 3.3 对RDD采用多个Task去重，提高并发度
    distinctRdd.distinct(2).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
distinct会存在shuffle过程<br>
2.3.1.12 coalesce()合并分区<br>
Coalesce算子包括：配置执行Shuffle和配置不执行Shuffle两种方式。<br>
1、不执行Shuffle方式<br>
1）函数签名：<br>
def coalesce(numPartitions: Int, shuffle: Boolean = false,  //默认false不执行shuffle<br>
partitionCoalescer: Option[PartitionCoalescer] = Option.empty)<br>
(implicit ord: Ordering[T] = null) : RDD[T]<br>
2）功能说明：缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。<br>
3）需求：4个分区合并为2个分区</p>
<p>4）分区源码</p>
<p>5）代码实现<br>
object value11_coalesce {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.创建一个RDD
    //val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 4)

    //3.1 缩减分区
    //val coalesceRdd: RDD[Int] = rdd.coalesce(2)

    //4. 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5, 6), 3)
    //4.1 缩减分区
    val coalesceRDD: RDD[Int] = rdd.coalesce(2)

    //5 查看对应分区数据
    val indexRDD: RDD[(Int, Int)] = coalesceRDD.mapPartitionsWithIndex(
        (index, datas) =&gt; {
            datas.map((index, _))
        }
    )

    //6 打印数据
    indexRDD.collect().foreach(println)
</code></pre>
<p>//8 延迟一段时间，观察http://localhost:4040页面，查看Shuffle读写数据<br>
Thread.sleep(100000)</p>
<pre><code>    //7.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2、执行Shuffle方式<br>
//3. 创建一个RDD<br>
val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5, 6), 3)<br>
//3.1 执行shuffle<br>
val coalesceRdd: RDD[Int] = rdd.coalesce(2, true)<br>
输出结果：<br>
(0,1)<br>
(0,4)<br>
(0,5)<br>
(1,2)<br>
(1,3)<br>
(1,6)<br>
3、Shuffle原理</p>
<p>2.3.1.13 repartition()重新分区（执行Shuffle）<br>
1）函数签名： def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]<br>
2）功能说明<br>
该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。<br>
3）需求说明：创建一个4个分区的RDD，对其重新分区。</p>
<p>4）代码实现<br>
object value12_repartition {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3. 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5, 6), 3)

    //3.1 缩减分区
    //val coalesceRdd: RDD[Int] = rdd.coalesce(2, true)

    //3.2 重新分区
    val repartitionRdd: RDD[Int] = rdd.repartition(2)

    //4 打印查看对应分区数据
    val indexRdd: RDD[(Int, Int)] = repartitionRdd.mapPartitionsWithIndex(
        (index, datas) =&gt; {
            datas.map((index, _))
        }
    )

    //5 打印
    indexRdd.collect().foreach(println)

    //6. 关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.14 coalesce和repartition区别<br>
1）coalesce重新分区，可以选择是否进行shuffle过程。由参数shuffle: Boolean = false/true决定。<br>
2）repartition实际上是调用的coalesce，进行shuffle。源码如下：<br>
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<br>
coalesce(numPartitions, shuffle = true)<br>
}<br>
3）coalesce一般为缩减分区，如果扩大分区，不使用shuffle是没有意义的，repartition扩大分区执行shuffle。<br>
2.3.1.15 sortBy()排序<br>
1）函数签名：<br>
def sortBy[K]( f: (T) =&gt; K,<br>
ascending: Boolean = true, // 默认为正序排列<br>
numPartitions: Int = this.partitions.length)<br>
(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>
2）功能说明<br>
该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为正序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。<br>
3）需求说明：创建一个RDD，按照数字大小分别实现正序和倒序排序</p>
<p>4）代码实现：<br>
object value13_sortBy {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    // 3.1 创建一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(2, 1, 3, 4, 6, 5))

    // 3.2 默认是升序排
    val sortRdd: RDD[Int] = rdd.sortBy(num =&gt; num)
    sortRdd.collect().foreach(println)

    // 3.3 配置为倒序排
    val sortRdd2: RDD[Int] = rdd.sortBy(num =&gt; num, false)
    sortRdd2.collect().foreach(println)

    // 3.4 创建一个RDD
    val strRdd: RDD[String] = sc.makeRDD(List(&quot;1&quot;, &quot;22&quot;, &quot;12&quot;, &quot;2&quot;, &quot;3&quot;))

    // 3.5 按照字符的int值排序
    strRdd.sortBy(num =&gt; num.toInt).collect().foreach(println)

    // 3.5 创建一个RDD
    val rdd3: RDD[(Int, Int)] = sc.makeRDD(List((2, 1), (1, 2), (1, 1), (2, 2)))

    // 3.6 先按照tuple的第一个值排序，相等再按照第2个值排
    rdd3.sortBy(t=&gt;t).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.1.16 pipe()调用脚本<br>
1）函数签名： def pipe(command: String): RDD[String]<br>
2）功能说明<br>
管道，针对每个分区，都调用一次shell脚本，返回输出的RDD。<br>
注意：在Worker节点可以访问到的位置脚本需要放</p>
<p>3）需求说明：编写一个脚本，使用管道将脚本作用于RDD上。<br>
（1）编写一个脚本，并增加执行权限<br>
[atguigu@hadoop102 spark-local]$ vim pipe.sh</p>
<p>#!/bin/bash</p>
<p>echo &quot;Start&quot;<br>
while read LINE; do<br>
echo &quot;&gt;&gt;&gt;&quot;${LINE}<br>
done</p>
<p>[atguigu@hadoop102 spark-local]$ chmod 777 pipe.sh<br>
（2）创建一个只有一个分区的RDD<br>
[atguigu@hadoop102 spark-local]$ bin/spark-shell</p>
<p>scala&gt; val rdd = sc.makeRDD (List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;), 1)<br>
（3）将脚本作用该RDD并打印<br>
scala&gt; rdd.pipe(&quot;/opt/module/spark-local/pipe.sh&quot;).collect()<br>
res18: Array[String] = Array(Start, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)<br>
（4）创建一个有两个分区的RDD<br>
scala&gt; val rdd = sc.makeRDD(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;), 2)<br>
（5）将脚本作用该RDD并打印<br>
scala&gt; rdd.pipe(&quot;/opt/module/spark-local/pipe.sh&quot;).collect()<br>
res19: Array[String] = Array(Start, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, Start, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)<br>
说明：一个分区调用一次脚本。<br>
2.3.2 双Value类型交互<br>
1）创建包名：com.atguigu.doublevalue<br>
2.3.2.1 intersection()交集<br>
1）函数签名：def intersection(other: RDD[T]): RDD[T]<br>
2）功能说明<br>
对源RDD和参数RDD求交集后返回一个新的RDD</p>
<p>交集：只有3<br>
3）需求说明：创建两个RDD，求两个RDD的交集</p>
<p>4）代码实现：<br>
object DoubleValue01_intersection {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd1: RDD[Int] = sc.makeRDD(1 to 4)

    //3.2 创建第二个RDD
    val rdd2: RDD[Int] = sc.makeRDD(4 to 8)

    //3.3 计算第一个RDD与第二个RDD的交集并打印
    rdd1.intersection(rdd2).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.2.2 union()并集不去重<br>
1）函数签名：def union(other: RDD[T]): RDD[T]<br>
2）功能说明<br>
对源RDD和参数RDD求并集后返回一个新的RDD</p>
<p>并集：1、2、3全包括<br>
3）需求说明：创建两个RDD，求并集</p>
<p>4）代码实现：<br>
object DoubleValue02_union {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd1: RDD[Int] = sc.makeRDD(1 to 4)

    //3.2 创建第二个RDD
    val rdd2: RDD[Int] = sc.makeRDD(4 to 8)

    //3.3 计算两个RDD的并集
    rdd1.union(rdd2).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.2.3 subtract()差集<br>
1）函数签名：def subtract(other: RDD[T]): RDD[T]<br>
2）功能说明<br>
计算差的一种函数，去除两个RDD中相同元素，不同的RDD将保留下来</p>
<p>差集：只有1<br>
3）需求说明：创建两个RDD，求第一个RDD与第二个RDD的差集</p>
<p>4）代码实现：<br>
object DoubleValue03_subtract {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(1 to 4)

    //3.2 创建第二个RDD
    val rdd1: RDD[Int] = sc.makeRDD(4 to 8)

    //3.3 计算第一个RDD与第二个RDD的差集并打印
    rdd.subtract(rdd1).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.2.4 zip()拉链<br>
1）函数签名：def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]<br>
2）功能说明<br>
该操作可以将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的元素。<br>
将两个RDD组合成Key/Value形式的RDD，这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。<br>
3）需求说明：创建两个RDD，并将两个RDD组合到一起形成一个(k,v)RDD</p>
<p>4）代码实现：<br>
object DoubleValue04_zip {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd1: RDD[Int] = sc.makeRDD(Array(1,2,3),3)

    //3.2 创建第二个RDD
    val rdd2: RDD[String] = sc.makeRDD(Array(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3)

    //3.3 第一个RDD组合第二个RDD并打印
    rdd1.zip(rdd2).collect().foreach(println)

    //3.4 第二个RDD组合第一个RDD并打印
    rdd2.zip(rdd1).collect().foreach(println)

    //3.5 创建第三个RDD（与1，2分区数不同）
    val rdd3: RDD[String] = sc.makeRDD(Array(&quot;a&quot;,&quot;b&quot;), 3)

    //3.6 元素个数不同，不能拉链
    // Can only zip RDDs with same number of elements in each partition
    rdd1.zip(rdd3).collect().foreach(println)

    //3.7 创建第四个RDD（与1，2分区数不同）
    val rdd4: RDD[String] = sc.makeRDD(Array(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), 2)

    //3.8 分区数不同，不能拉链
    // Can't zip RDDs with unequal numbers of partitions: List(3, 2)
    rdd1.zip(rdd4).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3 Key-Value类型<br>
1）创建包名：com.atguigu.keyvalue<br>
2.3.3.1 partitionBy()按照K重新分区<br>
1）函数签名：def partitionBy(partitioner: Partitioner): RDD[(K, V)]<br>
2）功能说明<br>
将RDD[K,V]中的K按照指定Partitioner重新进行分区；<br>
如果原有的RDD和新的RDD是一致的话就不进行分区，否则会产生Shuffle过程。<br>
3）需求说明：创建一个3个分区的RDD，对其重新分区</p>
<p>4）代码实现：<br>
object KeyValue01_partitionBy {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;)),3)

    //3.2 对RDD重新分区
    val rdd2: RDD[(Int, String)] = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))

    //3.3 打印查看对应分区数据  (0,(2,bbb))  (1,(1,aaa))  (1,(3,ccc))
    val indexRdd = rdd2.mapPartitionsWithIndex(
        (index, datas) =&gt; datas.map((index,_))
    )
    indexRdd.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.2 自定义分区<br>
1）HashPartitioner源码解读<br>
class HashPartitioner(partitions: Int) extends Partitioner {</p>
<pre><code>require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;)

def numPartitions: Int = partitions

def getPartition(key: Any): Int = key match {
    case null =&gt; 0
    case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)
}

override def equals(other: Any): Boolean = other match {
    case h: HashPartitioner =&gt;
        h.numPartitions == numPartitions
    case _ =&gt;
        false
}

override def hashCode: Int = numPartitions
</code></pre>
<p>}<br>
2）自定义分区器<br>
要实现自定义分区器，需要继承org.apache.spark.Partitioner类，并实现下面三个方法。<br>
（1）numPartitions: Int:返回创建出来的分区数。<br>
（2）getPartition(key: Any): Int:返回给定键的分区编号（0到numPartitions-1）。<br>
（3）equals():Java 判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同<br>
object KeyValue01_partitionBy {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, &quot;aaa&quot;), (2, &quot;bbb&quot;), (3, &quot;ccc&quot;)), 3)

    //3.2 自定义分区
    val rdd3: RDD[(Int, String)] = rdd.partitionBy(new MyPartitioner(2))

    //4 打印查看对应分区数据
    val indexRdd = rdd3.mapPartitionsWithIndex(
        (index, datas) =&gt; datas.map((index,_))
    )

    indexRdd.collect()

    //5.关闭连接
    sc.stop()
}
</code></pre>
<p>}</p>
<p>// 自定义分区<br>
class MyPartitioner(num: Int) extends Partitioner {</p>
<pre><code>// 设置的分区数
override def numPartitions: Int = num

// 具体分区逻辑
override def getPartition(key: Any): Int = {

    if (key.isInstanceOf[Int]) {

        val keyInt: Int = key.asInstanceOf[Int]
        if (keyInt % 2 == 0)
            0
        else
            1
    }else{
        0
    }
}
</code></pre>
<p>}<br>
2.3.3.3 reduceByKey()按照K聚合V<br>
1）函数签名：<br>
def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]<br>
def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]<br>
2）功能说明：该操作可以将RDD[K,V]中的元素按照相同的K对V进行聚合。其存在多种重载形式，还可以设置新RDD的分区数。<br>
3）需求说明：统计单词出现次数</p>
<p>4）代码实现：<br>
object KeyValue02_reduceByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd = sc.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,5),(&quot;a&quot;,5),(&quot;b&quot;,2)))

    //3.2 计算相同key对应值的相加结果
    val reduce: RDD[(String, Int)] = rdd.reduceByKey((v1,v2) =&gt; v1+v2)

    //3.3 打印结果
    reduce.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.4 groupByKey()按照K重新分组<br>
1）函数签名：def groupByKey(): RDD[(K, Iterable[V])]<br>
2）功能说明<br>
groupByKey对每个key进行操作，但只生成一个seq，并不进行聚合。<br>
该操作可以指定分区器或者分区数（默认使用HashPartitioner）<br>
3）需求说明：统计单词出现次数</p>
<p>4）代码实现：<br>
object KeyValue03_groupByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd = sc.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,5),(&quot;a&quot;,5),(&quot;b&quot;,2)))

    //3.2 将相同key对应值聚合到一个Seq中
    val group: RDD[(String, Iterable[Int])] = rdd.groupByKey()
    
    //3.3 打印结果
    group.collect().foreach(println)
    
    //3.4 计算相同key对应值的相加结果
    group.map(t=&gt;(t._1,t._2.sum)).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.5 reduceByKey和groupByKey区别<br>
1）reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[K,V]。<br>
2）groupByKey：按照key进行分组，直接进行shuffle。<br>
3）开发指导：在不影响业务逻辑的前提下，优先选用reduceByKey。求和操作不影响业务逻辑，求平均值影响业务逻辑。<br>
2.3.3.6 aggregateByKey()按照K处理分区内和分区间逻辑</p>
<p>2）需求分析</p>
<p>3）代码实现：<br>
object KeyValue04_aggregateByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;,1),(&quot;a&quot;,3),(&quot;a&quot;,5),(&quot;b&quot;,7),(&quot;b&quot;,2),(&quot;b&quot;,4),(&quot;b&quot;,6),(&quot;a&quot;,7)), 2)

    //3.2 取出每个分区相同key对应值的最大值，然后相加
    rdd.aggregateByKey(0)(math.max(_, _), _ + _).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.7 foldByKey()分区内和分区间相同的aggregateByKey()</p>
<p>4）代码实现：<br>
object KeyValue05_foldByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val list: List[(String, Int)] = List((&quot;a&quot;,1),(&quot;a&quot;,3),(&quot;a&quot;,5),(&quot;b&quot;,7),(&quot;b&quot;,2),(&quot;b&quot;,4),(&quot;b&quot;,6),(&quot;a&quot;,7))
    val rdd = sc.makeRDD(list,2)

    //3.2 求wordcount
    //rdd.aggregateByKey(0)(_+_,_+_).collect().foreach(println)

    rdd.foldByKey(0)(_+_).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.8 combineByKey()转换结构后分区内和分区间操作<br>
1）函数签名：<br>
def combineByKey[C](<br>
createCombiner: V =&gt; C,<br>
mergeValue: (C, V) =&gt; C,<br>
mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]<br>
（1）createCombiner（转换数据的结构）: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值<br>
（2）mergeValue（分区内）: 如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并<br>
（3）mergeCombiners（分区间）: 由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。<br>
2）功能说明<br>
针对相同K，将V合并成一个集合。<br>
3）需求说明：创建一个pairRDD，根据key计算每种key的平均值。（先计算每个key出现的次数以及可以对应值的总和，再相除得到结果）<br>
4）需求分析：</p>
<p>5）代码实现<br>
object KeyValue06_combineByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.1 创建第一个RDD
    val list: List[(String, Int)] = List((&quot;a&quot;, 88), (&quot;b&quot;, 95), (&quot;a&quot;, 91), (&quot;b&quot;, 93), (&quot;a&quot;, 95), (&quot;b&quot;, 98))
    val rdd: RDD[(String, Int)] = sc.makeRDD(list, 2)

    //3.2 将相同key对应的值相加，同时记录该key出现的次数，放入一个二元组
    val combineRdd: RDD[(String, (Int, Int))] = rdd.combineByKey(
        (_, 1),
        (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),
        (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)
    )

    //3.3 打印合并后的结果
    combineRdd.collect().foreach(println)

    //3.4 计算平均值
    combineRdd.map {
        case (key, value) =&gt; {
            (key, value._1 / value._2.toDouble)
        }
    }.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.9 reduceByKey、foldByKey、aggregateByKey、combineByKey</p>
<p>2.3.3.10 sortByKey()按照K进行排序<br>
1）函数签名：<br>
def sortByKey(<br>
ascending: Boolean = true, // 默认，升序<br>
numPartitions: Int = self.partitions.length)  : RDD[(K, V)]<br>
2）功能说明<br>
在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD<br>
3）需求说明：创建一个pairRDD，按照key的正序和倒序进行排序</p>
<p>4）代码实现：<br>
object KeyValue07_sortByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;)))

    //3.2 按照key的正序（默认顺序）
    rdd.sortByKey(true).collect().foreach(println)

    //3.3 按照key的倒序
    rdd.sortByKey(false).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.11 mapValues()只对V进行操作<br>
1）函数签名：def mapValues[U](f: V =&gt; U): RDD[(K, U)]<br>
2）功能说明：针对于(K,V)形式的类型只对V进行操作<br>
3）需求说明：创建一个pairRDD，并将value添加字符串&quot;|||&quot;</p>
<p>4）代码实现：<br>
object KeyValue08_mapValues {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, &quot;a&quot;), (1, &quot;d&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)))

    //3.2 对value添加字符串&quot;|||&quot;
    rdd.mapValues(_ + &quot;|||&quot;).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.12 join()等同于sql里的内连接,关联上的要,关联不上的舍弃<br>
1）函数签名：<br>
def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]<br>
def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]<br>
2）功能说明<br>
在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD<br>
3）需求说明：创建两个pairRDD，并将key相同的数据聚合到一个元组。</p>
<p>4）代码实现：<br>
object KeyValue09_join {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)))

    //3.2 创建第二个pairRDD
    val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (4, 6)))

    //3.3 join操作并打印结果
    rdd.join(rdd1).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.3.13 cogroup()类似于sql的全连接，但是在同一个RDD中对key聚合<br>
1）函数签名：def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]<br>
2）功能说明<br>
在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD<br>
操作两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合。<br>
3）需求说明：创建两个pairRDD，并将key相同的数据聚合到一个迭代器。</p>
<p>4）代码实现：<br>
object KeyValue10_cogroup {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))

    //3.2 创建第二个RDD
    val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1,4),(2,5),(4,6)))

    //3.3 cogroup两个RDD并打印结果
</code></pre>
<p>// (1,(CompactBuffer(a),CompactBuffer(4)))<br>
// (2,(CompactBuffer(b),CompactBuffer(5)))<br>
// (3,(CompactBuffer(c),CompactBuffer()))<br>
// (4,(CompactBuffer(),CompactBuffer(6)))<br>
rdd.cogroup(rdd1).collect().foreach(println)</p>
<pre><code>    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.3.4 案例实操（省份广告被点击Top3）<br>
0）数据准备：时间戳，省份，城市，用户，广告，中间字段使用空格分割。</p>
<p>3）实现过程<br>
object Demo_ad_click_top3 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1. 初始化Spark配置信息并建立与Spark的连接
    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkCoreTest&quot;)
    val sc = new SparkContext(sparkConf)

    //2. 读取日志文件，获取原始数据
    val dataRDD: RDD[String] = sc.textFile(&quot;input/agent.log&quot;)

    //3. 将原始数据进行结构转换string =&gt;(prv-adv,1)
    val prvAndAdvToOneRDD: RDD[(String, Int)] = dataRDD.map {
        line =&gt; {
            val datas: Array[String] = line.split(&quot; &quot;)
            (datas(1) + &quot;-&quot; + datas(4), 1)
        }
    }

    //4. 将转换结构后的数据进行聚合统计（prv-adv,1）=&gt;(prv-adv,sum)
    val prvAndAdvToSumRDD: RDD[(String, Int)] = prvAndAdvToOneRDD.reduceByKey(_ + _)

    //5. 将统计的结果进行结构的转换（prv-adv,sum）=&gt;(prv,(adv,sum))
    val prvToAdvAndSumRDD: RDD[(String, (String, Int))] = prvAndAdvToSumRDD.map {
        case (prvAndAdv, sum) =&gt; {
            val ks: Array[String] = prvAndAdv.split(&quot;-&quot;)
            (ks(0), (ks(1), sum))
        }
    }

    //6. 根据省份对数据进行分组：(prv,(adv,sum)) =&gt; (prv, Iterator[(adv,sum)])
    val groupRDD: RDD[(String, Iterable[(String, Int)])] = prvToAdvAndSumRDD.groupByKey()

    //7. 对相同省份中的广告进行排序（降序），取前三名
    val mapValuesRDD: RDD[(String, List[(String, Int)])] = groupRDD.mapValues {
        datas =&gt; {
            datas.toList.sortWith(
                (left, right) =&gt; {
                    left._2 &gt; right._2
                }
            ).take(3)
        }
    }

    //8. 将结果打印
    mapValuesRDD.collect().foreach(println)

    //9.关闭与spark的连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4 Action行动算子<br>
行动算子是触发了整个作业的执行。因为转换算子都是懒加载，并不会立即执行。<br>
1）创建包名：com.atguigu.action<br>
2.4.1 reduce()聚合<br>
1）函数签名：def reduce(f: (T, T) =&gt; T): T<br>
2）功能说明：f函数聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据。</p>
<p>3）需求说明：创建一个RDD，将所有元素聚合得到结果<br>
object action01_reduce {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 聚合数据
    val reduceResult: Int = rdd.reduce(_+_)
    println(reduceResult)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.2 collect()以数组的形式返回数据集<br>
1）函数签名：def collect(): Array[T]<br>
2）功能说明：在驱动程序中，以数组Array的形式返回数据集的所有元素。</p>
<p>注意：所有的数据都会被拉取到Driver端，慎用<br>
3）需求说明：创建一个RDD，并将RDD内容收集到Driver端打印<br>
object action02_collect {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 收集数据到Driver
    rdd.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.3 count()返回RDD中元素个数<br>
1）函数签名：def count(): Long<br>
2）功能说明：返回RDD中元素的个数</p>
<p>3）需求说明：创建一个RDD，统计该RDD的条数<br>
object action03_count {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 返回RDD中元素的个数
    val countResult: Long = rdd.count()
    println(countResult)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.4 first()返回RDD中的第一个元素<br>
1）函数签名：def first(): T<br>
2）功能说明：返回RDD中的第一个元素</p>
<p>3）需求说明：创建一个RDD，返回该RDD中的第一个元素<br>
object action04_first {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 返回RDD中元素的个数
    val firstResult: Int = rdd.first()
    println(firstResult)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.5 take()返回由RDD前n个元素组成的数组<br>
1）函数签名：def take(num: Int): Array[T]<br>
2）功能说明：返回一个由RDD的前n个元素组成的数组</p>
<p>3）需求说明：创建一个RDD，取出前两个元素<br>
object action05_take {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 返回RDD中前2个元素
    val takeResult: Array[Int] = rdd.take(2)
    println(takeResult.mkString(&quot;,&quot;))

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.6 takeOrdered()返回该RDD排序后前n个元素组成的数组<br>
1）函数签名：def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]<br>
2）功能说明：返回该RDD排序后的前n个元素组成的数组</p>
<p>def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {<br>
......<br>
if (mapRDDs.partitions.length == 0) {<br>
Array.empty<br>
} else {<br>
mapRDDs.reduce { (queue1, queue2) =&gt;<br>
queue1 ++= queue2<br>
queue1<br>
}.toArray.sorted(ord)<br>
}<br>
}<br>
3）需求说明：创建一个RDD，获取该RDD排序后的前2个元素<br>
object action06_takeOrdered{</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,3,2,4))

    //3.2 返回RDD中排完序后的前两个元素
    val result: Array[Int] = rdd.takeOrdered(2)
    println(result.mkString(&quot;,&quot;))

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.7 aggregate()案例</p>
<p>3）需求说明：创建一个RDD，将所有元素相加得到结果<br>
object action07_aggregate {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 8)

    //3.2 将该RDD所有元素相加得到结果
    //val result: Int = rdd.aggregate(0)(_ + _, _ + _)
    val result: Int = rdd.aggregate(10)(_ + _, _ + _)

    println(result)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.8 fold()案例</p>
<p>3）需求说明：创建一个RDD，将所有元素相加得到结果<br>
object action08_fold {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))

    //3.2 将该RDD所有元素相加得到结果
    val foldResult: Int = rdd.fold(0)(_+_)
    println(foldResult)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.9 countByKey()统计每种key的个数<br>
1）函数签名：def countByKey(): Map[K, Long]<br>
2）功能说明：统计每种key的个数</p>
<p>3）需求说明：创建一个PairRDD，统计每种key的个数<br>
object action09_countByKey {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[(Int, String)] = sc.makeRDD(List((1, &quot;a&quot;), (1, &quot;a&quot;), (1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;), (3, &quot;c&quot;)))

    //3.2 统计每种key的个数
    val result: collection.Map[Int, Long] = rdd.countByKey()
    println(result)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.10 save相关算子<br>
1）saveAsTextFile(path)保存成Text文件<br>
（1）函数签名<br>
（2）功能说明：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本<br>
2）saveAsSequenceFile(path) 保存成Sequencefile文件<br>
（1）函数签名<br>
（2）功能说明：将数据集中的元素以Hadoop Sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。<br>
注意：只有kv类型RDD有该操作，单值的没有<br>
3）saveAsObjectFile(path) 序列化成对象保存到文件<br>
（1）函数签名<br>
（2）功能说明：用于将RDD中的元素序列化成对象，存储到文件中。<br>
4）代码实现<br>
object action10_save {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4), 2)

    //3.2 保存成Text文件
    rdd.saveAsTextFile(&quot;output&quot;)

    //3.3 序列化成对象保存到文件
    rdd.saveAsObjectFile(&quot;output1&quot;)

    //3.4 保存成Sequencefile文件
    rdd.map((_,1)).saveAsSequenceFile(&quot;output2&quot;)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.4.11 foreach(f)遍历RDD中每一个元素</p>
<p>3）需求说明：创建一个RDD，对每个元素进行打印<br>
object action11_foreach {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3具体业务逻辑
    //3.1 创建第一个RDD
    // val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4),2)
    val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

    //3.2 收集后打印
    rdd.collect().foreach(println)

    println(&quot;****************&quot;)

    //3.3 分布式打印
    rdd.foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.5 RDD序列化<br>
在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要注意的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。下面我们看几个例子：</p>
<p>2.5.1 闭包检查<br>
0）创建包名：com.atguigu.serializable<br>
1）闭包引入（有闭包就需要进行序列化）<br>
object serializable01_object {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.创建两个对象
    val user1 = new User()
    user1.name = &quot;zhangsan&quot;

    val user2 = new User()
    user2.name = &quot;lisi&quot;

    val userRDD1: RDD[User] = sc.makeRDD(List(user1, user2))

    //3.1 打印，ERROR报java.io.NotSerializableException
    //userRDD1.foreach(user =&gt; println(user.name))
    

    //3.2 打印，RIGHT （因为没有传对象到Executor端）
    val userRDD2: RDD[User] = sc.makeRDD(List())
    //userRDD2.foreach(user =&gt; println(user.name))

    //3.3 打印，ERROR Task not serializable 
</code></pre>
<p>//注意：此段代码没执行就报错了,因为spark自带闭包检查<br>
userRDD2.foreach(user =&gt; println(user.name+&quot; love &quot;+user1.name))</p>
<pre><code>    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}</p>
<p>//case class User() {<br>
//    var name: String = _<br>
//}<br>
class User extends Serializable {<br>
var name: String = _<br>
}<br>
2.5.2 序列化方法和属性<br>
1）说明<br>
Driver：算子以外的代码都是在Driver端执行<br>
Executor：算子里面的代码都是在Executor端执行<br>
2）代码实现<br>
object serializable02_function {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.创建一个RDD
    val rdd: RDD[String] = sc.makeRDD(Array(&quot;hello world&quot;, &quot;hello spark&quot;, &quot;hive&quot;, &quot;atguigu&quot;))

    //3.1创建一个Search对象
    val search = new Search(&quot;hello&quot;)

    // Driver：算子以外的代码都是在Driver端执行
    // Executor：算子里面的代码都是在Executor端执行
    //3.2 函数传递，打印：ERROR Task not serializable
    search.getMatch1(rdd).collect().foreach(println)

    //3.3 属性传递，打印：ERROR Task not serializable
    search.getMatche2(rdd).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}</p>
<p>class Search(query:String) extends Serializable {</p>
<pre><code>def isMatch(s: String): Boolean = {
    s.contains(query)
}

// 函数序列化案例
def getMatch1 (rdd: RDD[String]): RDD[String] = {
    //rdd.filter(this.isMatch)
    rdd.filter(isMatch)
}

// 属性序列化案例
def getMatche2(rdd: RDD[String]): RDD[String] = {
    //rdd.filter(x =&gt; x.contains(this.query))
    rdd.filter(x =&gt; x.contains(query))
    //val q = query
    //rdd.filter(x =&gt; x.contains(q))
}
</code></pre>
<p>}<br>
3）问题一说明<br>
//过滤出包含字符串的RDD<br>
def getMatch1 (rdd: RDD[String]): RDD[String] = {<br>
rdd.filter(isMatch)<br>
}<br>
（1）在这个方法中所调用的方法isMatch()是定义在Search这个类中的，实际上调用的是this. isMatch()，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。<br>
（2）解决方案<br>
类继承scala.Serializable即可。<br>
class Search() extends Serializable{...}<br>
4）问题二说明<br>
//过滤出包含字符串的RDD<br>
def getMatche2(rdd: RDD[String]): RDD[String] = {<br>
rdd.filter(x =&gt; x.contains(query))<br>
}<br>
（1）在这个方法中所调用的方法query是定义在Search这个类中的字段，实际上调用的是this. query，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。<br>
（2）解决方案一<br>
（a）类继承scala.Serializable即可。<br>
class Search() extends Serializable{...}<br>
（b）将类变量query赋值给局部变量<br>
修改getMatche2为<br>
//过滤出包含字符串的RDD<br>
def getMatche2(rdd: RDD[String]): RDD[String] = {<br>
val q = this.query//将类变量赋值给局部变量<br>
rdd.filter(x =&gt; x.contains(q))<br>
}<br>
（3）解决方案二<br>
把Search类变成样例类，样例类默认是序列化的。<br>
case class Search(query:String) {...}<br>
2.5.3 Kryo序列化框架<br>
参考地址: https://github.com/EsotericSoftware/kryo<br>
Java的序列化能够序列化任何的类。但是比较重，序列化后对象的体积也比较大。<br>
Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。<br>
注意：即使使用Kryo序列化，也要继承Serializable接口或者使用样例类。<br>
object serializable03_Kryo {</p>
<pre><code>def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf()
            .setAppName(&quot;SerDemo&quot;)
            .setMaster(&quot;local[*]&quot;)
            // 替换默认的序列化机制
            .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
            // 注册需要使用kryo序列化的自定义类
            .registerKryoClasses(Array(classOf[Searche]))

    val sc = new SparkContext(conf)

    val rdd: RDD[String] = sc.makeRDD(Array(&quot;hello world&quot;, &quot;hello atguigu&quot;, &quot;atguigu&quot;, &quot;hahah&quot;), 2)

    val searche = new Searche(&quot;hello&quot;)
    val result: RDD[String] = searche.getMatchedRDD1(rdd)

    result.collect.foreach(println)
}
</code></pre>
<p>}</p>
<p>class Searche(val query: String) extends Serializable{</p>
<pre><code>def isMatch(s: String) = {
    s.contains(query)
}

def getMatchedRDD1(rdd: RDD[String]) = {
    rdd.filter(isMatch) 
}

def getMatchedRDD2(rdd: RDD[String]) = {
    rdd.filter(_.contains(this.query))
}
</code></pre>
<p>}<br>
2.6 RDD依赖关系<br>
2.6.1 查看血缘关系<br>
RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<p>0）创建包名：com.atguigu.dependency<br>
1）代码实现<br>
object Lineage01 {</p>
<p>def main(args: Array[String]): Unit = {</p>
<pre><code>    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
    println(fileRDD.toDebugString)
    println(&quot;----------------------&quot;)

    val wordRDD: RDD[String] = fileRDD.flatMap(_.split(&quot; &quot;))
    println(wordRDD.toDebugString)
    println(&quot;----------------------&quot;)

    val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))
    println(mapRDD.toDebugString)
    println(&quot;----------------------&quot;)

    val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)
    println(resultRDD.toDebugString)

    resultRDD.collect()

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2）打印结果<br>
(2) input/1.txt MapPartitionsRDD[1] at textFile at Lineage01.scala:15 []</p>
<table>
<thead>
<tr>
<th>input/1.txt HadoopRDD[0] at textFile at Lineage01.scala:15 []</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>(2) MapPartitionsRDD[2] at flatMap at Lineage01.scala:19 []<br>
|  input/1.txt MapPartitionsRDD[1] at textFile at Lineage01.scala:15 []</p>
<table>
<thead>
<tr>
<th>input/1.txt HadoopRDD[0] at textFile at Lineage01.scala:15 []</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>(2) MapPartitionsRDD[3] at map at Lineage01.scala:23 []<br>
|  MapPartitionsRDD[2] at flatMap at Lineage01.scala:19 []<br>
|  input/1.txt MapPartitionsRDD[1] at textFile at Lineage01.scala:15 []</p>
<table>
<thead>
<tr>
<th>input/1.txt HadoopRDD[0] at textFile at Lineage01.scala:15 []</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>(2) ShuffledRDD[4] at reduceByKey at Lineage01.scala:27 []<br>
+-(2) MapPartitionsRDD[3] at map at Lineage01.scala:23 []<br>
|  MapPartitionsRDD[2] at flatMap at Lineage01.scala:19 []<br>
|  input/1.txt MapPartitionsRDD[1] at textFile at Lineage01.scala:15 []<br>
|  input/1.txt HadoopRDD[0] at textFile at Lineage01.scala:15 []<br>
注意：圆括号中的数字表示RDD的并行度，也就是有几个分区<br>
2.6.2 查看依赖关系</p>
<p>1）代码实现<br>
object Lineage02 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
    println(fileRDD.dependencies)
    println(&quot;----------------------&quot;)

    val wordRDD: RDD[String] = fileRDD.flatMap(_.split(&quot; &quot;))
    println(wordRDD.dependencies)
    println(&quot;----------------------&quot;)

    val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))
    println(mapRDD.dependencies)
    println(&quot;----------------------&quot;)

    val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)
    println(resultRDD.dependencies)

    resultRDD.collect()

    // 查看localhost:4040页面，观察DAG图
</code></pre>
<p>Thread.sleep(10000000)</p>
<pre><code>    //4.关闭连接
    sc.stop()
}
</code></pre>
<h2 id="2打印结果listorgapachesparkonetoonedependencyf2ce6b">}<br>
2）打印结果<br>
List(org.apache.spark.OneToOneDependency@f2ce6b)</h2>
<h2 id="listorgapachesparkonetoonedependency692fd26">List(org.apache.spark.OneToOneDependency@692fd26)</h2>
<h2 id="listorgapachesparkonetoonedependency627d8516">List(org.apache.spark.OneToOneDependency@627d8516)</h2>
<p>List(org.apache.spark.ShuffleDependency@a518813)<br>
3）全局搜索（ctrl+n）org.apache.spark.OneToOneDependency<br>
class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency<a href="rdd">T</a> {<br>
override def getParents(partitionId: Int): List[Int] = List(partitionId)<br>
}<br>
注意：要想理解RDDS是如何工作的，最重要的就是理解Transformations。<br>
RDD之间的关系可以从两个维度来理解：一个是RDD是从哪些RDD转换而来，也就是 RDD的parent RDD(s)是什么(血缘); 另一个就是RDD依赖于parent RDD(s)的哪些Partition(s)，这种关系就是RDD之间的依赖(依赖)。<br>
RDD和它依赖的父RDD（s）的依赖关系有两种不同的类型，即窄依赖（NarrowDependency）和宽依赖（ShuffleDependency）。<br>
2.6.3 窄依赖<br>
窄依赖表示每一个父RDD的Partition最多被子RDD的一个Partition使用(一对一or多对一)，窄依赖我们形象的比喻为独生子女。</p>
<p>2.6.4 宽依赖<br>
宽依赖表示同一个父RDD的Partition被多个子RDD的Partition依赖(只能是一对多)，会引起Shuffle，总结：宽依赖我们形象的比喻为超生。</p>
<p>具有宽依赖的transformations包括：sort、reduceByKey、groupByKey、join和调用rePartition函数的任何操作。<br>
宽依赖对Spark去评估一个transformations有更加重要的影响，比如对性能的影响。<br>
在不影响业务要求的情况下，要尽量避免使用有宽依赖的转换算子，因为有宽依赖,就一定会走shuffle，影响性能<br>
2.6.5 Stage任务划分（面试重点）<br>
1）DAG有向无环图<br>
DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG记录了RDD的转换过程和任务的阶段。</p>
<p>2）任务运行的整体流程</p>
<p>3）RDD任务切分中间分为：Application、Job、Stage和Task<br>
（1）Application：初始化一个SparkContext即生成一个Application；<br>
（2）Job：一个Action算子就会生成一个Job；<br>
（3）Stage：Stage等于宽依赖的个数加1；<br>
（4）Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。<br>
注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。</p>
<p>4）代码实现<br>
object Lineage03 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
//Application：初始化一个SparkContext即生成一个Application<br>
val sc = new SparkContext(conf)</p>
<pre><code>//textFile,flatMap,map算子全部是窄依赖,不会增加stage阶段
val lineRDD: RDD[String] = sc.textFile(&quot;D:\\IdeaProjects\\SparkCoreTest\\input\\1.txt&quot;)
val flatMapRDD: RDD[String] = lineRDD.flatMap(_.split(&quot; &quot;))
val mapRDD: RDD[(String, Int)] = flatMapRDD.map((_, 1))

//reduceByKey算子会有宽依赖,stage阶段加1，2个stage
val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_ + _)

//Job：一个Action算子就会生成一个Job，2个Job
//job0打印到控制台
resultRDD.collect().foreach(println)
//job1输出到磁盘
resultRDD.saveAsTextFile(&quot;D:\\IdeaProjects\\SparkCoreTest\\out&quot;)

//阻塞线程,方便进入localhost:4040查看
Thread.sleep(Long.MaxValue)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
5）查看Job个数<br>
查看http://localhost:4040/jobs/，发现Job有两个。</p>
<p>6）查看Stage个数<br>
查看Job0的Stage。由于只有1个Shuffle阶段，所以Stage个数为2。</p>
<p>查看Job1的Stage。由于只有1个Shuffle阶段，所以Stage个数为2。</p>
<p>7）Task个数<br>
查看Job0的Stage0的Task个数，2个</p>
<p>查看Job0的Stage1的Task个数，2个</p>
<p>查看Job1的Stage2的Task个数，0个（2个跳过skipped）</p>
<p>查看Job1的Stage3的Task个数，2个</p>
<p>注意：如果存在shuffle过程，系统会自动进行缓存，UI界面显示skipped的部分<br>
2.6.6 Stage任务划分源码分析</p>
<p>2.7 RDD持久化<br>
2.7.1 RDD Cache缓存<br>
RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以序列化的形式缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p>0）创建包名：com.atguigu.cache<br>
1）代码实现<br>
object cache01 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3. 创建一个RDD，读取指定位置文件:hello atguigu atguigu
    val lineRdd: RDD[String] = sc.textFile(&quot;input1&quot;)

    //3.1.业务逻辑
    val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))

    val wordToOneRdd: RDD[(String, Int)] = wordRdd.map {
        word =&gt; {
            println(&quot;************&quot;)
            (word, 1)
        }
    }

    //3.5 cache缓存前打印血缘关系
    println(wordToOneRdd.toDebugString)

    //3.4 数据缓存。
</code></pre>
<p>//cache底层调用的就是persist方法,缓存级别默认用的是MEMORY_ONLY<br>
wordToOneRdd.cache()</p>
<pre><code>    //3.6 persist方法可以更改存储级别
    // wordToOneRdd.persist(StorageLevel.MEMORY_AND_DISK_2)

    //3.2 触发执行逻辑
    wordToOneRdd.collect().foreach(println)
    
    //3.5 cache缓存后打印血缘关系
</code></pre>
<p>//cache操作会增加血缘关系，不改变原有的血缘关系<br>
println(wordToOneRdd.toDebugString)</p>
<pre><code>    println(&quot;==================================&quot;)
    
    //3.3 再次触发执行逻辑
    wordToOneRdd.collect().foreach(println)

    Thread.sleep(1000000)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2）源码解析<br>
mapRdd.cache()<br>
def cache(): this.type = persist()<br>
def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</p>
<p>object StorageLevel {<br>
val NONE = new StorageLevel(false, false, false, false)<br>
val DISK_ONLY = new StorageLevel(true, false, false, false)<br>
val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)<br>
val MEMORY_ONLY = new StorageLevel(false, true, false, true)<br>
val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)<br>
val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)<br>
val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)<br>
val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)<br>
val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)<br>
val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)<br>
val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)<br>
val OFF_HEAP = new StorageLevel(true, true, true, false, 1)<br>
注意：默认的存储级别都是仅在内存存储一份。在存储级别的末尾加上“_2”表示持久化的数据存为两份。SER：表示序列化。</p>
<p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。<br>
3）自带缓存算子<br>
Spark会自动对一些Shuffle操作的中间数据做持久化操作（比如：reduceByKey）。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。<br>
object cache02 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3. 创建一个RDD，读取指定位置文件:hello atguigu atguigu
    val lineRdd: RDD[String] = sc.textFile(&quot;input1&quot;)

    //3.1.业务逻辑
    val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))

    val wordToOneRdd: RDD[(String, Int)] = wordRdd.map {
        word =&gt; {
            println(&quot;************&quot;)
            (word, 1)
        }
    }

    // 采用reduceByKey，自带缓存
    val wordByKeyRDD: RDD[(String, Int)] = wordToOneRdd.reduceByKey(_+_)

    //3.5 cache操作会增加血缘关系，不改变原有的血缘关系
    println(wordByKeyRDD.toDebugString)

    //3.4 数据缓存。
    //wordByKeyRDD.cache()

    //3.2 触发执行逻辑
    wordByKeyRDD.collect()

    println(&quot;-----------------&quot;)
    println(wordByKeyRDD.toDebugString)

    //3.3 再次触发执行逻辑
    wordByKeyRDD.collect()

    Thread.sleep(1000000)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
访问http://localhost:4040/jobs/页面，查看第一个和第二个job的DAG图。说明：增加缓存后血缘依赖关系仍然有，但是，第二个job取的数据是从缓存中取的。</p>
<p>2.7.2 RDD CheckPoint检查点<br>
1）检查点：是通过将RDD中间结果写入磁盘。<br>
2）为什么要做检查点？<br>
由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。<br>
3）检查点存储路径：Checkpoint的数据通常是存储在HDFS等容错、高可用的文件系统<br>
4）检查点数据存储格式为：二进制的文件<br>
5）检查点切断血缘：在Checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移除。<br>
6）检查点触发时间：对RDD进行Checkpoint操作并不会马上被执行，必须执行Action操作才能触发。但是检查点为了数据安全，会从血缘关系的最开始执行一遍。</p>
<p>7）设置检查点步骤<br>
（1）设置检查点数据存储路径：sc.setCheckpointDir(&quot;./checkpoint1&quot;)<br>
（2）调用检查点方法：wordToOneRdd.checkpoint()<br>
8）代码实现<br>
object checkpoint01 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    // 需要设置路径，否则抛异常：Checkpoint directory has not been set in the SparkContext
    sc.setCheckpointDir(&quot;./checkpoint1&quot;)

    //3. 创建一个RDD，读取指定位置文件:hello atguigu atguigu
    val lineRdd: RDD[String] = sc.textFile(&quot;input1&quot;)

    //3.1.业务逻辑
    val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))

    val wordToOneRdd: RDD[(String, Long)] = wordRdd.map {
        word =&gt; {
            (word, System.currentTimeMillis())
        }
    }

    //3.5 增加缓存，避免再重新跑一个job做checkpoint
</code></pre>
<p>//        wordToOneRdd.cache()</p>
<pre><code>    //3.4 数据检查点：针对wordToOneRdd做检查点计算
    wordToOneRdd.checkpoint()

    //3.2 触发执行逻辑
    wordToOneRdd.collect().foreach(println)
    // 会立即启动一个新的job来专门的做checkpoint运算

    //3.3 再次触发执行逻辑
    wordToOneRdd.collect().foreach(println)
    wordToOneRdd.collect().foreach(println)

    Thread.sleep(10000000)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
9）执行结果<br>
访问http://localhost:4040/jobs/页面，查看4个job的DAG图。其中第2个图是checkpoint的job运行DAG图。第3、4张图说明，检查点切断了血缘依赖关系。</p>
<pre><code>（1）只增加checkpoint，没有增加Cache缓存打印
第1个job执行完，触发了checkpoint，第2个job运行checkpoint，并把数据存储在检查点上。第3、4个job，数据从检查点上直接读取。
</code></pre>
<p>(hadoop,1577960215526)<br>
。。。。。。<br>
(hello,1577960215526)<br>
(hadoop,1577960215609)<br>
。。。。。。<br>
(hello,1577960215609)<br>
(hadoop,1577960215609)<br>
。。。。。。<br>
(hello,1577960215609)<br>
（2）增加checkpoint，也增加Cache缓存打印<br>
第1个job执行完，数据就保存到Cache里面了，第2个job运行checkpoint，直接读取Cache里面的数据，并把数据存储在检查点上。第3、4个job，数据从检查点上直接读取。<br>
(hadoop,1577960642223)<br>
。。。。。。<br>
(hello,1577960642225)<br>
(hadoop,1577960642223)<br>
。。。。。。<br>
(hello,1577960642225)<br>
(hadoop,1577960642223)<br>
。。。。。。<br>
(hello,1577960642225)</p>
<p>2.7.3 缓存和检查点区别<br>
1）Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。<br>
2）Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。<br>
3）建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。<br>
4）如果使用完了缓存，可以通过unpersist()方法释放缓存<br>
2.7.4 检查点存储到HDFS集群<br>
如果检查点数据存储到HDFS集群，要注意配置访问集群的用户名。否则会报访问权限异常。<br>
object checkpoint02 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    // 设置访问HDFS集群的用户名
    System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;atguigu&quot;)

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    // 需要设置路径.需要提前在HDFS集群上创建/checkpoint路径
    sc.setCheckpointDir(&quot;hdfs://hadoop102:8020/checkpoint&quot;)

    //3. 创建一个RDD，读取指定位置文件:hello atguigu atguigu
    val lineRdd: RDD[String] = sc.textFile(&quot;input1&quot;)

    //3.1.业务逻辑
    val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))

    val wordToOneRdd: RDD[(String, Long)] = wordRdd.map {
        word =&gt; {
            (word, System.currentTimeMillis())
        }
    }

    //3.4 增加缓存，避免再重新跑一个job做checkpoint
    wordToOneRdd.cache()

    //3.3 数据检查点：针对wordToOneRdd做检查点计算
    wordToOneRdd.checkpoint()

    //3.2 触发执行逻辑
    wordToOneRdd.collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.8 键值对RDD数据分区<br>
Spark目前支持Hash分区、Range分区和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区和Reduce的个数。<br>
1）注意：<br>
（1）只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None<br>
（2）每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。<br>
2）获取RDD分区<br>
（1）创建包名：com.atguigu.partitioner<br>
（2）代码实现<br>
object partitioner01_get {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3 创建RDD
    val pairRDD: RDD[(Int, Int)] = sc.makeRDD(List((1,1),(2,2),(3,3)))

    //3.1 打印分区器
    println(pairRDD.partitioner)

    //3.2 使用HashPartitioner对RDD进行重新分区
    val partitionRDD: RDD[(Int, Int)] = pairRDD.partitionBy(new HashPartitioner(2))

    //3.3 打印分区器
    println(partitionRDD.partitioner)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2.8.1 Hash分区</p>
<p>2.8.2 Ranger分区</p>
<p>2.8.3 自定义分区<br>
详见2.3.3.2。<br>
第3章 数据读取与保存<br>
Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。<br>
文件格式分为：Text文件、Sequence文件以及Object文件；<br>
文件系统分为：本地文件系统、HDFS以及数据库。<br>
3.1 文件类数据读取与保存<br>
1）创建包名：com.atguigu.readAndSave<br>
3.1.1 Text文件<br>
1）基本语法<br>
（1）数据读取：textFile(String)<br>
（2）数据保存：saveAsTextFile(String)<br>
2）代码实现<br>
object Operate_Text {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[1]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.1 读取输入文件
    val inputRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)

    //3.2 保存数据
    inputRDD.saveAsTextFile(&quot;textFile&quot;)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
4）注意：如果是集群路径：hdfs://hadoop102:8020/input/1.txt<br>
3.1.2 Sequence文件<br>
SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile<a href="path">keyClass, valueClass</a>。<br>
1）代码实现<br>
object Operate_Sequence {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[1]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.1 创建rdd
    val dataRDD: RDD[(Int, Int)] = sc.makeRDD(Array((1,2),(3,4),(5,6)))

    //3.2 保存数据为SequenceFile
    dataRDD.saveAsSequenceFile(&quot;seqFile&quot;)

    //3.3 读取SequenceFile文件
    sc.sequenceFile[Int,Int](&quot;seqFile&quot;).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
2）注意：SequenceFile文件只针对PairRDD<br>
3.1.3 Object对象文件<br>
对象文件是将对象序列化后保存的文件，采用hadoop的序列化机制。可以通过objectFile<a href="path">k,v</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为要序列化所以要指定类型。<br>
1）代码实现<br>
object Operate_Object {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[1]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.1 创建RDD
    val dataRDD: RDD[Int] = sc.makeRDD(Array(1,2,3,4),2)

    //3.2 保存数据
    dataRDD.saveAsObjectFile(&quot;objFile&quot;)

    //3.3 读取数据
    sc.objectFile[Int](&quot;objFile&quot;).collect().foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
3.2 文件系统类数据读取与保存<br>
Spark的整个生态系统与Hadoop是完全兼容的，所以对于Hadoop所支持的文件类型或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套创建操作接口。如TextInputFormat，新旧两个版本所引用分别是org.apache.hadoop.mapred.InputFormat、org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)<br>
第4章 累加器<br>
累加器：分布式共享只写变量。（Executor和Executor之间不能读数据）<br>
累加器用来把Executor端变量信息聚合到Driver端。在Driver中定义的一个变量，在Executor端的每个task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行合并计算。</p>
<p>4.1 系统累加器<br>
1）累加器使用<br>
（1）累加器定义（SparkContext.accumulator(initialValue)方法）<br>
val sum: LongAccumulator = sc.longAccumulator(&quot;sum&quot;)<br>
（2）累加器添加数据（累加器.add方法）<br>
sum.add(count)<br>
（3）累加器获取数据（累加器.value）<br>
sum.value<br>
2）创建包名：com.atguigu.accumulator<br>
3）代码实现<br>
object accumulator01_system {<br>
package com.atguigu.cache<br>
import org.apache.spark.rdd.RDD<br>
import org.apache.spark.util.LongAccumulator<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object accumulator01_system {<br>
def main(args: Array[String]): Unit = {<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
val sc = new SparkContext(conf)</p>
<pre><code>val dataRDD: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 3), (&quot;a&quot;, 4)))
//需求:统计a出现的所有次数 (&quot;a&quot;,10)

//普通算子实现 reduceByKey 代码会走shuffle 效率低
//val rdd1: RDD[(String, Int)] = dataRDD.reduceByKey(_ + _)
</code></pre>
<p>//普通变量无法实现<br>
//结论:普通变量只能从driver端发给executor端,在executor计算完以后,结果不会返回给driver端<br>
/*<br>
var sum = 0</p>
<pre><code>dataRDD.foreach{
  case (a,count) =&gt; {
    sum += count
    println(&quot;sum = &quot; + sum)
  }
}

println((&quot;a&quot;,sum))
</code></pre>
<p>*/<br>
//累加器实现<br>
//1 声明累加器<br>
val accSum: LongAccumulator = sc.longAccumulator(&quot;sum&quot;)</p>
<pre><code>dataRDD.foreach{
  case (a,count) =&gt; {
    //2 使用累加器累加  累加器.add()
    accSum.add(count)
    // 4 不要在executor端获取累加器的值,因为不准确 
</code></pre>
<p>//因此我们说累加器叫分布式共享只写变量<br>
//println(&quot;sum = &quot; + accSum.value)<br>
}<br>
}<br>
//3 获取累加器的值 累加器.value<br>
println((&quot;a&quot;,accSum.value))</p>
<pre><code>sc.stop()
</code></pre>
<p>}<br>
}<br>
注意：Executor端的任务不能读取累加器的值（例如：在Executor端调用sum.value，获取的值不是累加器最终的值）。因此我们说，累加器是一个分布式共享只写变量。<br>
3）累加器要放在行动算子中<br>
因为转换算子执行的次数取决于job的数量，如果一个spark应用有多个行动算子，那么转换算子中的累加器可能会发生不止一次更新，导致结果错误。所以，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动算子中。<br>
对于在行动算子中使用的累加器，Spark只会把每个Job对各累加器的修改应用一次。<br>
object accumulator02_updateCount {<br>
def main(args: Array[String]): Unit = {<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
val sc = new SparkContext(conf)</p>
<pre><code>val dataRDD: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 3), (&quot;a&quot;, 4)))
//需求:统计a出现的所有次数 (&quot;a&quot;,10)
//累加器实现
//1 声明累加器
val accSum: LongAccumulator = sc.longAccumulator(&quot;sum&quot;)

val mapRDD: RDD[Unit] = dataRDD.map {
  case (a, count) =&gt; {
    //2 使用累加器累加  累加器.add()
    accSum.add(count)
    // 4 不要在executor端获取累加器的值,因为不准确 因此我们说累加器叫分布式共享只写变量
    //println(&quot;sum = &quot; + accSum.value)
  }
}

//调用两次行动算子，map执行两次，导致最终累加器的值翻倍
mapRDD.collect()
mapRDD.collect()

/**
 * 结论:使用累加器最好要在行动算子中使用,因为行动算子只会执行一次,而转换算子的执行次数不确定!
 */ 
//2 获取累加器的值 累加器.value
println((&quot;a&quot;,accSum.value))

sc.stop()
</code></pre>
<p>}<br>
}<br>
4.2 自定义累加器<br>
自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。<br>
1）自定义累加器步骤<br>
（1）继承AccumulatorV2，设定输入、输出泛型<br>
（2）重写6个抽象方法<br>
（3）使用自定义累加器需要注册:：sc.register(累加器,&quot;累加器名字&quot;)<br>
2）需求：自定义累加器，统计RDD中首字母为“H”的单词以及出现的次数。<br>
List(&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Spark&quot;, &quot;Spark&quot;)</p>
<p>3）代码实现<br>
package com.atguigu.accumulator</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.util.AccumulatorV2<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>import scala.collection.mutable</p>
<p>object accumulator03_define {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3. 创建RDD
    val rdd: RDD[String] = sc.makeRDD(List(&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Spark&quot;, &quot;Spark&quot;), 2)

    //3.1 创建累加器
    val acc: MyAccumulator = new MyAccumulator()

    //3.2 注册累加器
    sc.register(acc,&quot;wordcount&quot;)

    //3.3 使用累加器
    rdd.foreach(
        word =&gt;{
            acc.add(word)
        }
    )

    //3.4 获取累加器的累加结果
    println(acc.value)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}</p>
<p>// 自定义累加器<br>
// 1.继承AccumulatorV2,设定输入、输出泛型<br>
// 2.重写方法<br>
class MyAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]] {</p>
<pre><code>// 定义输出数据集合,一个可变的Map
var map = mutable.Map[String, Long]()

// 是否为初始化状态，如果集合数据为空，即为初始化状态
override def isZero: Boolean = map.isEmpty

// 复制累加器
override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = {
    new MyAccumulator()
}

// 重置累加器
override def reset(): Unit = map.clear()

// 增加数据
override def add(v: String): Unit = {
    // 业务逻辑
    if (v.startsWith(&quot;H&quot;)) {
        map(v) = map.getOrElse(v, 0L) + 1L
    }
}

// 合并累加器
override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = {

    other.value.foreach{
        case (word, count) =&gt;{
            map(word) = map.getOrElse(word, 0L) + count
        }
    }
}

// 累加器的值，其实就是累加器的返回结果
override def value: mutable.Map[String, Long] = map
</code></pre>
<p>}<br>
第5章 广播变量<br>
广播变量：分布式共享只读变量。<br>
广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark Task操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来会很顺手。在多个Task并行操作中使用同一个变量，但是Spark会为每个Task任务分别发送。<br>
1）使用广播变量步骤：<br>
（1）调用SparkContext.broadcast（广播变量）创建出一个广播对象，任何可序列化的类型都可以这么实现。<br>
（2）通过广播变量.value，访问该对象的值。<br>
（3）广播变量只会被发到各个节点一次，作为只读值处理（修改这个值不会影响到别的节点）。<br>
2）原理说明<br>
3）创建包名：com.atguigu.broadcast<br>
4）代码实现<br>
object broadcast01 {</p>
<pre><code>def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    //3.创建一个字符串RDD，过滤出包含WARN的数据
    val rdd: RDD[String] = sc.makeRDD(List(&quot;WARN:Class Not Find&quot;, &quot;INFO:Class Not Find&quot;, &quot;DEBUG:Class Not Find&quot;), 4)
    val str: String = &quot;WARN&quot;

    // 声明广播变量
    val bdStr: Broadcast[String] = sc.broadcast(str)

    val filterRDD: RDD[String] = rdd.filter {
        // log=&gt;log.contains(str)
        log =&gt; log.contains(bdStr.value)
    }

    filterRDD.foreach(println)

    //4.关闭连接
    sc.stop()
}
</code></pre>
<p>}<br>
第6章 SparkCore实战<br>
6.1 数据准备<br>
1）数据格式</p>
<p>2）数据详细字段说明<br>
编号	字段名称	字段类型	字段含义<br>
1	date	String	用户点击行为的日期<br>
2	user_id	Long	用户的ID<br>
3	session_id	String	Session的ID<br>
4	page_id	Long	某个页面的ID<br>
5	action_time	String	动作的时间点<br>
6	search_keyword	String	用户搜索的关键词<br>
7	click_category_id	Long	点击某一个商品品类的ID<br>
8	click_product_id	Long	某一个商品的ID<br>
9	order_category_ids	String	一次订单中所有品类的ID集合<br>
10	order_product_ids	String	一次订单中所有商品的ID集合<br>
11	pay_category_ids	String	一次支付中所有品类的ID集合<br>
12	pay_product_ids	String	一次支付中所有商品的ID集合<br>
13	city_id	Long	城市 id<br>
6.2 需求1：Top10热门品类</p>
<p>需求说明：品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量(次数)来统计热门品类。<br>
鞋			点击数 下单数  支付数<br>
衣服		点击数 下单数  支付数<br>
电脑		点击数 下单数  支付数<br>
例如，综合排名 = 点击数<em>20% + 下单数</em>30% + 支付数*50%<br>
本项目需求优化为：先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。<br>
6.2.1 需求分析（方案一）常规算子<br>
思路：分别统计每个品类点击的次数，下单的次数和支付的次数。然后想办法将三个RDD联合到一起。<br>
（品类，点击总数）（品类，下单总数）（品类，支付总数）<br>
（品类，（点击总数，下单总数，支付总数））<br>
然后就可以按照各品类的元组（点击总数，下单总数，支付总数）进行倒序排序了，因为元组排序刚好是先排第一个元素，然后排第二个元素，最后第三个元素。<br>
6.2.2 需求实现（方案一）<br>
0）创建包名：com.atguigu.project01<br>
1）方案一代码实现方式1（cogroup算子实现满外连接）<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object require01_top10Category_method1 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)<br>
//1.读取原始日志数据<br>
val actionRDD: RDD[String] = sc.textFile(&quot;input\user_visit_action.txt&quot;)<br>
//2.统计品类的点击数量 (品类ID,点击数量)<br>
val clickActionRDD: RDD[String] = actionRDD.filter {<br>
action =&gt; {<br>
val datas: Array[String] = action.split(&quot;<em>&quot;)<br>
datas(6) != &quot;-1&quot;<br>
}<br>
}<br>
val clickCountRDD: RDD[(String, Int)] = clickActionRDD.map {<br>
action =&gt; {<br>
val datas: Array[String] = action.split(&quot;</em>&quot;)<br>
(datas(6), 1)<br>
}<br>
}.reduceByKey(_ + _)</p>
<pre><code>//3.统计品类的下单数量 (品类ID,下单数量)
val orderActionRDD: RDD[String] = actionRDD.filter {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    datas(8) != &quot;null&quot;
  }
}
val orderCountRDD: RDD[(String, Int)] = orderActionRDD.flatMap {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    val cids: Array[String] = datas(8).split(&quot;,&quot;)
    cids.map((_, 1))
  }
}.reduceByKey(_ + _)

//4.统计品类的支付数量 (品类ID,支付数量)
val payActionRDD: RDD[String] = actionRDD.filter {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    datas(10) != &quot;null&quot;
  }
}
val payCountRDD: RDD[(String, Int)] = payActionRDD.flatMap {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    val cids: Array[String] = datas(10).split(&quot;,&quot;)
    cids.map((_, 1))
  }
}.reduceByKey(_ + _)

//5.按照品类进行排序,取热门品类前10名
//热门排名:先按照点击排,然后按照下单数量排,最后按照支付数量排
//元组排序:先比较第一个,再比较第二个,最后比较第三个
//(品类ID,(点击数量,下单数量,支付数量))
//三个RDD需要进行满外连接,因此需要用到cogroup()算子
val cogroupRDD: RDD[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = clickCountRDD.cogroup(orderCountRDD, payCountRDD)

val cogroupRDD2: RDD[(String, (Int, Int, Int))] = cogroupRDD.mapValues {
  case (iter1, iter2, iter3) =&gt; {
    var clickCnt =0
    val clickIter: Iterator[Int] = iter1.iterator
    if (clickIter.hasNext) {
      clickCnt = clickIter.next()
    }

    var orderCnt =0
    val orderIter: Iterator[Int] = iter2.iterator
    if (orderIter.hasNext) {
      orderCnt = orderIter.next()
    }

    var payCnt =0
    val payIter: Iterator[Int] = iter3.iterator
    if (payIter.hasNext) {
      payCnt = payIter.next()
    }

    (clickCnt,orderCnt,payCnt)
  }
}
//倒序排序取前10
val result: Array[(String, (Int, Int, Int))] = cogroupRDD2.sortBy(_._2, false).take(10)

//6.将结果采集到控制到打印输出
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
上述方案有两个小问题需要解决优化<br>
问题1：actionRDD算子用到了多次，最好提前缓存下。<br>
问题2：cogroup算子底层会走shuffle，效率比较低。<br>
2）方案一代码实现方式2（union补0的方式实现满外连接）<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object require01_top10Category_method1_2 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)<br>
//1.读取原始日志数据<br>
val actionRDD: RDD[String] = sc.textFile(&quot;input\user_visit_action.txt&quot;)</p>
<pre><code>//问题1:actionRDD在下面用到了多次,最好缓存一下
actionRDD.cache()

//2.统计品类的点击数量 (品类ID,点击数量)
val clickActionRDD: RDD[String] = actionRDD.filter {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    datas(6) != &quot;-1&quot;
  }
}
val clickCountRDD: RDD[(String, Int)] = clickActionRDD.map {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    (datas(6), 1)
  }
}.reduceByKey(_ + _)

//3.统计品类的下单数量 (品类ID,下单数量)
val orderActionRDD: RDD[String] = actionRDD.filter {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    datas(8) != &quot;null&quot;
  }
}
val orderCountRDD: RDD[(String, Int)] = orderActionRDD.flatMap {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    val cids: Array[String] = datas(8).split(&quot;,&quot;)
    cids.map((_, 1))
  }
}.reduceByKey(_ + _)

//4.统计品类的支付数量 (品类ID,支付数量)
val payActionRDD: RDD[String] = actionRDD.filter {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    datas(10) != &quot;null&quot;
  }
}
val payCountRDD: RDD[(String, Int)] = payActionRDD.flatMap {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    val cids: Array[String] = datas(10).split(&quot;,&quot;)
    cids.map((_, 1))
  }
}.reduceByKey(_ + _)

//问题2:cogroup()算子底层会走shuffle,最好换掉..
//5.按照品类进行排序,取热门品类前10名
//union方式实现满外连接，需要先补0
// (品类ID,点击数量)  =&gt; (品类ID,(点击数量,0,0))
// (品类ID,下单数量)  =&gt; (品类ID,(0,下单数量,0))
// (品类ID,支付数量)  =&gt; (品类ID,(0,0,支付数量))
// (品类ID,(点击数量,下单数量,支付数量))
val clickRDD: RDD[(String, (Int, Int, Int))] = clickCountRDD.map {
  case (cid, cnt) =&gt; {
    (cid, (cnt, 0, 0))
  }
}
val ordrRDD: RDD[(String, (Int, Int, Int))] = orderCountRDD.map {
  case (cid, cnt) =&gt; {
    (cid, (0, cnt, 0))
  }
}
val payRDD: RDD[(String, (Int, Int, Int))] = payCountRDD.map {
  case (cid, cnt) =&gt; {
    (cid, (0, 0, cnt))
  }
}
//union的方式实现满外连接
val unionRDD: RDD[(String, (Int, Int, Int))] = clickRDD.union(ordrRDD).union(payRDD)

val unionRDD2: RDD[(String, (Int, Int, Int))] = unionRDD.reduceByKey {
  (t1, t2) =&gt; {
    (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)
  }
}

//倒序排序取前10
val result: Array[(String, (Int, Int, Int))] = unionRDD2.sortBy(_._2, false).take(10)

//6.将结果采集到控制到打印输出
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
上述方案还可以继续优化…生命不止,优化不息…<br>
3）方案一代码实现方式3（转换数据结构，提前补0，减少reduceByKey的次数）<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object require01_top10Category_method1_3 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)<br>
//1.读取原始日志数据<br>
val actionRDD: RDD[String] = sc.textFile(&quot;input\user_visit_action.txt&quot;)</p>
<pre><code>//2.将数据转换结构 (提前补0)
// 如果数据是点击 : (品类ID,(1,0,0))
// 如果数据是下单 : (品类ID,(0,1,0))
// 如果数据是支付 : (品类ID,(0,0,1))
val flatRDD: RDD[(String, (Int, Int, Int))] = actionRDD.flatMap {
  action =&gt; {
    val datas: Array[String] = action.split(&quot;_&quot;)
    if (datas(6) != &quot;-1&quot;) {
      //点击的数据
      List((datas(6), (1, 0, 0)))
    } else if (datas(8) != &quot;null&quot;) {
      //下单的数据
      val ids: Array[String] = datas(8).split(&quot;,&quot;)
      ids.map((_, (0, 1, 0)))
    } else if (datas(10) != &quot;null&quot;) {
      //支付的数据
      val ids: Array[String] = datas(10).split(&quot;,&quot;)
      ids.map((_, (0, 0, 1)))
    } else {
      Nil
    }
  }
}

//3.将相同的品类ID的数据进行分组聚合 reduceByKey
val reduceRDD: RDD[(String, (Int, Int, Int))] = flatRDD.reduceByKey {
  (t1, t2) =&gt; {
    (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)
  }
}

//倒序排序取前10
val result: Array[(String, (Int, Int, Int))] = reduceRDD.sortBy(_._2, false).take(10)

//4.将结果采集到控制台打印输出
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}</p>
<p>6.2.3 需求分析（方案二）样例类<br>
采用样例类的方式实现。</p>
<p>6.2.4 需求实现（方案二）<br>
1）用来封装用户行为的样例类<br>
//用户访问动作表<br>
case class UserVisitAction(date: String,//用户点击行为的日期<br>
user_id: String,//用户的ID<br>
session_id: String,//Session的ID<br>
page_id: String,//某个页面的ID<br>
action_time: String,//动作的时间点<br>
search_keyword: String,//用户搜索的关键词<br>
click_category_id: String,//某一个商品品类的ID<br>
click_product_id: String,//某一个商品的ID<br>
order_category_ids: String,//一次订单中所有品类的ID集合<br>
order_product_ids: String,//一次订单中所有商品的ID集合<br>
pay_category_ids: String,//一次支付中所有品类的ID集合<br>
pay_product_ids: String,//一次支付中所有商品的ID集合<br>
city_id: String)//城市 id<br>
// 输出结果表<br>
case class CategoryCountInfo(categoryId: String,//品类id<br>
clickCount: Long,//点击次数<br>
orderCount: Long,//订单次数<br>
payCount: Long)//支付次数<br>
注意：样例类的属性默认是val修饰，不能修改；需要修改属性，需要采用var修饰。<br>
// 输出结果表<br>
case class CategoryCountInfo(var categoryId: String,//品类id<br>
var clickCount: Long,//点击次数<br>
var orderCount: Long,//订单次数<br>
var payCount: Long)//支付次数<br>
2）核心业务代码实现<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object require01_top10Category_method2 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)</p>
<pre><code>//1 读取数据
val lineRDD: RDD[String] = sc.textFile(&quot;input\\user_visit_action.txt&quot;)
//2 封装样例类 将lineRDD变为actionRDD
val actionRDD: RDD[UserVisitAction] = lineRDD.map(
  line =&gt; {
    val datas: Array[String] = line.split(&quot;_&quot;)
    //将解析出来的数据封装到样例类里面
    UserVisitAction(
      datas(0),
      datas(1),
      datas(2),
      datas(3),
      datas(4),
      datas(5),
      datas(6),
      datas(7),
      datas(8),
      datas(9),
      datas(10),
      datas(11),
      datas(12)
    )
  }
)
//3 转换数据结构 将actionRDD的数据变为 CategoryCountInfo(品类ID,1,0,0)
val infoRDD: RDD[CategoryCountInfo] = actionRDD.flatMap(
  action =&gt; {
    if (action.click_category_id != &quot;-1&quot;) {
      //点击的数据
      List(CategoryCountInfo(action.click_category_id, 1, 0, 0))
    } else if (action.order_category_ids != &quot;null&quot;) {
      //下单的数据
      val arr: Array[String] = action.order_category_ids.split(&quot;,&quot;)
      arr.map(id =&gt; CategoryCountInfo(id, 0, 1, 0))
    } else if (action.pay_category_ids != &quot;null&quot;) {
      //支付的数据
      val arr: Array[String] = action.pay_category_ids.split(&quot;,&quot;)
      arr.map(id =&gt; CategoryCountInfo(id, 0, 0, 1))
    } else {
      Nil
    }
  }
)
//4 按照品类id分组,将同一个品类的数据分到同一个组内
val groupRDD: RDD[(String, Iterable[CategoryCountInfo])] = infoRDD.groupBy(_.categoryId)

//5 将分组后的数据进行聚合处理 (品类id, (品类id, clickCount, OrderCount, PayCount))
val reduceRDD: RDD[CategoryCountInfo] = groupRDD.mapValues(
  datas =&gt; {
    datas.reduce(
      (info1, info2) =&gt; {
        info1.orderCount += info2.orderCount
        info1.clickCount += info2.clickCount
        info1.payCount += info2.payCount
        info1
      }
    )
  }
).map(_._2)

//6 将聚合后的数据 倒序排序 取前10
val result: Array[CategoryCountInfo] = reduceRDD.sortBy(info =&gt; (info.clickCount, info.orderCount, info.payCount), false).take(10)
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
6.2.5 需求分析（方案三）样例类+算子优化<br>
针对方案二中的groupBy算子，没有提前聚合的功能，替换成reduceByKey</p>
<p>6.2.6 需求实现（方案三）<br>
1）样例类代码和方案二一样。（详见方案二）<br>
2）核心代码实现<br>
object require01_top10Category_method3 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)</p>
<pre><code>//1 读取数据
val lineRDD: RDD[String] = sc.textFile(&quot;input\\user_visit_action.txt&quot;)
//2 封装样例类 将lineRDD变为actionRDD
val actionRDD: RDD[UserVisitAction] = lineRDD.map(
  line =&gt; {
    val datas: Array[String] = line.split(&quot;_&quot;)
    //将解析出来的数据封装到样例类里面
    UserVisitAction(
      datas(0),
      datas(1),
      datas(2),
      datas(3),
      datas(4),
      datas(5),
      datas(6),
      datas(7),
      datas(8),
      datas(9),
      datas(10),
      datas(11),
      datas(12)
    )
  }
)
//3 转换数据结构 将actionRDD的数据变为 CategoryCountInfo(品类ID,1,0,0)
val infoRDD: RDD[(String, CategoryCountInfo)] = actionRDD.flatMap(
  action =&gt; {
    if (action.click_category_id != &quot;-1&quot;) {
      //点击的数据
      List((action.click_category_id, CategoryCountInfo(action.click_category_id, 1, 0, 0)))
    } else if (action.order_category_ids != &quot;null&quot;) {
      //下单的数据
      val arr: Array[String] = action.order_category_ids.split(&quot;,&quot;)
      arr.map(id =&gt; (id, CategoryCountInfo(id, 0, 1, 0)))
    } else if (action.pay_category_ids != &quot;null&quot;) {
      //支付的数据
      val arr: Array[String] = action.pay_category_ids.split(&quot;,&quot;)
      arr.map(id =&gt; (id, CategoryCountInfo(id, 0, 0, 1)))
    } else {
      Nil
    }
  }
)
// 4 按照品类id分组,两两聚合
val reduceRDD: RDD[CategoryCountInfo] = infoRDD.reduceByKey(
  (info1, info2) =&gt; {
    info1.orderCount += info2.orderCount
    info1.clickCount += info2.clickCount
    info1.payCount += info2.payCount
    info1
  }
).map(_._2)

//5 将聚合后的数据 倒序排序 取前10
val result: Array[CategoryCountInfo] = reduceRDD.sortBy(info =&gt; (info.clickCount, info.orderCount, info.payCount), false).take(10)
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
6.2.7 需求分析（方案四）累加器</p>
<p>6.2.8 需求实现（方案四）<br>
1）累加器实现<br>
//品类行为统计累加器<br>
/**</p>
<ul>
<li>输入  UserVisitAction</li>
<li>输出  ((鞋,click),1) ((鞋,order),1) ((鞋,pay),1)  mutable.Map[(String, String), Long]<br>
*/<br>
class CategoryCountAccumulator extends AccumulatorV2[UserVisitAction,mutable.Map[(String, String), Long]]{<br>
var map = mutable.Map<a href="">(String, String), Long</a></li>
</ul>
<p>override def isZero: Boolean = map.isEmpty</p>
<p>override def copy(): AccumulatorV2[UserVisitAction, mutable.Map[(String, String), Long]] = new CategoryCountAccumulator</p>
<p>override def reset(): Unit = map.clear()</p>
<p>override def add(action: UserVisitAction): Unit = {<br>
if(action.click_category_id != &quot;-1&quot;){<br>
//点击  key=(cid,&quot;click&quot;)<br>
val key = (action.click_category_id,&quot;click&quot;)<br>
map(key) = map.getOrElse(key,0L) + 1L<br>
}else if(action.order_category_ids != &quot;null&quot;){<br>
//下单  key=(cid,&quot;order&quot;)<br>
val cids: Array[String] = action.order_category_ids.split(&quot;,&quot;)<br>
for (cid &lt;- cids) {<br>
val key = (cid,&quot;order&quot;)<br>
map(key) = map.getOrElse(key,0L) + 1L<br>
}</p>
<pre><code>}else if(action.pay_category_ids != &quot;null&quot;){
  //支付  key=(cid,&quot;pay&quot;)
  val cids: Array[String] = action.pay_category_ids.split(&quot;,&quot;)
  for (cid &lt;- cids) {
    val key = (cid,&quot;pay&quot;)
    map(key) = map.getOrElse(key,0L) + 1L
  }
}
</code></pre>
<p>}</p>
<p>override def merge(other: AccumulatorV2[UserVisitAction, mutable.Map[(String, String), Long]]): Unit = {<br>
other.value.foreach {<br>
case (key, count) =&gt; {<br>
map(key) = map.getOrElse(key, 0L) + count<br>
}<br>
}<br>
}</p>
<p>override def value: mutable.Map[(String, String), Long] = map<br>
}<br>
2）核心逻辑实现<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.util.AccumulatorV2<br>
import org.apache.spark.{SparkConf, SparkContext}<br>
import scala.collection.{immutable, mutable}</p>
<p>object require01_top10Category_method4 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)</p>
<pre><code>//1 读取数据
val lineRDD: RDD[String] = sc.textFile(&quot;input\\user_visit_action.txt&quot;)
//2 封装样例类 将lineRDD变为actionRDD
val actionRDD: RDD[UserVisitAction] = lineRDD.map(
  line =&gt; {
    val datas: Array[String] = line.split(&quot;_&quot;)
    //将解析出来的数据封装到样例类里面
    UserVisitAction(
      datas(0),
      datas(1),
      datas(2),
      datas(3),
      datas(4),
      datas(5),
      datas(6),
      datas(7),
      datas(8),
      datas(9),
      datas(10),
      datas(11),
      datas(12)
    )
  }
)
//3 使用累加器统计相同品类id的点击数量,下单数量,支付数量
//3.1 创建累加器
val cateAcc = new CategoryCountAccumulator
//3.2 注册累加器
sc.register(cateAcc,&quot;cateacc&quot;)
//3.3 使用累加器
actionRDD.foreach(action =&gt; cateAcc.add(action))
//3.4 获得累加器的值 ((4,click),5961) ((4,order),1760) ((4,pay),1271)
val accMap: mutable.Map[(String, String), Long] = cateAcc.value

//4 将accMap按照品类id进行分组(4,Map((4,click) -&gt; 5961, (4,order) -&gt; 1760, (4,pay) -&gt; 1271))
val groupMap: Map[String, mutable.Map[(String, String), Long]] = accMap.groupBy(_._1._1)

//5 将groupMap转换成样例类集合
val infoIter: immutable.Iterable[CategoryCountInfo] = groupMap.map {
  case (id, map) =&gt; {
    val click = map.getOrElse((id, &quot;click&quot;), 0L)
    val order = map.getOrElse((id, &quot;order&quot;), 0L)
    val pay = map.getOrElse((id, &quot;pay&quot;), 0L)
    CategoryCountInfo(id, click, order, pay)
  }
}
//6 对样例类集合倒序排序取前10
val result: List[CategoryCountInfo] = infoIter.toList.sortBy(info =&gt; (info.clickCount, info.orderCount, info.payCount))(Ordering[(Long, Long, Long)].reverse).take(10)
result.foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
6.3 需求2：Top10热门品类中每个品类的Top10活跃Session统计<br>
6.3.1 需求分析</p>
<p>6.3.2 需求实现<br>
1）累加器实现 参考6.2.7章节<br>
2）核心逻辑实现<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.broadcast.Broadcast<br>
import org.apache.spark.{SparkConf, SparkContext}<br>
import org.apache.spark.rdd.RDD<br>
import scala.collection.{immutable, mutable}</p>
<p>object require02_top10Category_sessionTop10 {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)</p>
<pre><code>//1 读取数据
val lineRDD: RDD[String] = sc.textFile(&quot;input\\user_visit_action.txt&quot;)
//2 封装样例类 将lineRDD变为actionRDD
val actionRDD: RDD[UserVisitAction] = lineRDD.map(
  line =&gt; {
    val datas: Array[String] = line.split(&quot;_&quot;)
    //将解析出来的数据封装到样例类里面
    UserVisitAction(
      datas(0),
      datas(1),
      datas(2),
      datas(3),
      datas(4),
      datas(5),
      datas(6),
      datas(7),
      datas(8),
      datas(9),
      datas(10),
      datas(11),
      datas(12)
    )
  }
)
//3 使用累加器统计相同品类id的点击数量,下单数量,支付数量
//3.1 创建累加器
val cateAcc = new CategoryCountAccumulator

//3.2 注册累加器
sc.register(cateAcc, &quot;cateacc&quot;)

//3.3 使用累加器
actionRDD.foreach(action =&gt; cateAcc.add(action))

//3.4 获得累加器的值 ((4,click),5961) ((4,order),1760) ((4,pay),1271)
val accMap: mutable.Map[(String, String), Long] = cateAcc.value

//4 将accMap按照品类id进行分组   (4,Map((4,click) -&gt; 5961, (4,order) -&gt; 1760, (4,pay) -&gt; 1271))
val groupMap: Map[String, mutable.Map[(String, String), Long]] = accMap.groupBy(_._1._1)

//5 将groupMap转换成样例类集合
val infoIter: immutable.Iterable[CategoryCountInfo] = groupMap.map {
  case (id, map) =&gt; {
    val click = map.getOrElse((id, &quot;click&quot;), 0L)
    val order = map.getOrElse((id, &quot;order&quot;), 0L)
    val pay = map.getOrElse((id, &quot;pay&quot;), 0L)
    CategoryCountInfo(id, click, order, pay)
  }
}

//6 对样例类集合倒序排序取前10
val result: List[CategoryCountInfo] = infoIter.toList.sortBy(info =&gt; (info.clickCount, info.orderCount, info.payCount))(Ordering[(Long, Long, Long)].reverse).take(10)

//********************需求二**********************
//1 获取Top10热门品类
val ids: List[String] = result.map(_.categoryId)
//2 ids创建广播变量
val bdIds: Broadcast[List[String]] = sc.broadcast(ids)
//3 过滤原始数据,保留热门Top10的点击数据
val filterActionRDD: RDD[UserVisitAction] = actionRDD.filter(
  action =&gt; {
    //一个会话一定会从点击开始,因此我们在这只要点击的数据
    if (action.click_category_id != &quot;-1&quot;) {
      bdIds.value.contains(action.click_category_id)
    } else {
      false
    }
  }
)
//4 转换数据结构 UserVisitAction =&gt; (品类id-会话id,1)
val idAndSessionToOneRDD: RDD[(String, Int)] = filterActionRDD.map(
  action =&gt; (action.click_category_id + &quot;=&quot; + action.session_id, 1)
)

//5 按照品类id-会话id分组聚合 (品类id-会话id,1)=&gt;(品类id-会话id,sum)
val idAndSessionToSumRDD: RDD[(String, Int)] = idAndSessionToOneRDD.reduceByKey(_ + _)

//6 再次变化结构,分开品类和会话 (品类id-会话id,sum) =&gt; (品类id,(会话id,sum))
val idToSessionAndSumRDD: RDD[(String, (String, Int))] = idAndSessionToSumRDD.map {
  case (key, sum) =&gt; {
    val keys: Array[String] = key.split(&quot;=&quot;)
    (keys(0), (keys(1), sum))
  }
}

//7 按照品类id分组 (品类id,(会话id,sum)) =&gt; (品类id,[(会话id,sum) ,(会话id,sum),....])
val groupRDD: RDD[(String, Iterable[(String, Int)])] = idToSessionAndSumRDD.groupByKey()

//8 对groupRDD的每个品类的集合倒序排序,求前10
val resultRDD: RDD[(String, List[(String, Int)])] = groupRDD.mapValues(
  datas =&gt; {
    datas.toList.sortBy(_._2)(Ordering[Int].reverse)
    }.take(10)
)

resultRDD.collect().foreach(println)

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}<br>
6.4 需求3：页面单跳转化率统计<br>
6.4.1 需求分析<br>
1）页面单跳转化率<br>
计算页面单跳转化率，什么是页面单跳转换率，比如一个用户在一次 Session 过程中访问的页面路径 3,5,7,9,10,21，那么页面 3 跳到页面 5 叫一次单跳，7-9 也叫一次单跳，那么单跳转化率就是要统计页面点击的概率。<br>
比如：计算 3-5 的单跳转化率，先获取符合条件的 Session 对于页面 3 的访问次数（PV）为 A，然后获取符合条件的 Session 中访问了页面 3 又紧接着访问了页面 5 的次数为 B，那么 B/A 就是 3-5 的页面单跳转化率。</p>
<p>2）统计页面单跳转化率意义<br>
产品经理和运营总监，可以根据这个指标，去尝试分析，整个网站，产品，各个页面的表现怎么样，是不是需要去优化产品的布局；吸引用户最终可以进入最后的支付页面。<br>
数据分析师，可以此数据做更深一步的计算和分析。<br>
企业管理层，可以看到整个公司的网站，各个页面的之间的跳转的表现如何，可以适当调整公司的经营战略或策略。<br>
3）需求详细描述<br>
在该模块中，需要根据查询对象中设置的Session过滤条件，先将对应得Session过滤出来，然后根据查询对象中设置的页面路径，计算页面单跳转化率，比如查询的页面路径为：3、5、7、8，那么就要计算3-5、5-7、7-8的页面单跳转化率。<br>
需要注意的一点是，页面的访问是有先后的，要做好排序。<br>
1、2、3、4、5、6、7<br>
1-2/ 1   2-3/2   3-4/3   4-5/4    5-6/5    6-7/6<br>
4）需求分析</p>
<p>6.4.2 需求实现<br>
1）代码实现<br>
package com.atguigu.project01</p>
<p>import org.apache.spark.rdd.RDD<br>
import org.apache.spark.{SparkConf, SparkContext}</p>
<p>object require03_PageFlow {<br>
def main(args: Array[String]): Unit = {<br>
//TODO 1 创建SparkConf配置文件,并设置App名称<br>
val conf = new SparkConf().setAppName(&quot;SparkCoreTest&quot;).setMaster(&quot;local[*]&quot;)<br>
//TODO 2 利用SparkConf创建sc对象<br>
val sc = new SparkContext(conf)<br>
//1 读取数据<br>
val lineRDD: RDD[String] = sc.textFile(&quot;input\user_visit_action.txt&quot;)<br>
//2 封装样例类 将lineRDD变为actionRDD<br>
val actionRDD: RDD[UserVisitAction] = lineRDD.map(<br>
line =&gt; {<br>
val datas: Array[String] = line.split(&quot;_&quot;)<br>
//将解析出来的数据封装到样例类里面<br>
UserVisitAction(<br>
datas(0),<br>
datas(1),<br>
datas(2),<br>
datas(3),<br>
datas(4),<br>
datas(5),<br>
datas(6),<br>
datas(7),<br>
datas(8),<br>
datas(9),<br>
datas(10),<br>
datas(11),<br>
datas(12)<br>
)<br>
}<br>
)<br>
//3 准备过滤数据<br>
//定义需要统计的页面 (只统计集合中规定的页面跳转率) 分母过滤<br>
val ids = List(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;)<br>
//定义对分子过滤的集合<br>
val zipIds: List[String] = ids.zip(ids.tail).map {<br>
case (p1, p2) =&gt; p1 + &quot;-&quot; + p2<br>
}</p>
<pre><code>//对ids创建广播变量
val bdIds = sc.broadcast(ids)

//4 计算分母 (页面,访问次数)
val idsArr = actionRDD
  //过滤出要统计的page_id(由于最后一个页面总次数，不参与运算，所以也过滤了)
  .filter(action =&gt; bdIds.value.init.contains(action.page_id))
  .map(action =&gt; (action.page_id, 1))
  .reduceByKey(_ + _).collect()

//array转成Map方便后续使用
val idsMap: Map[String, Int] = idsArr.toMap

//5 计算分子
//5.1 按照session进行分组
val sessionGroupRDD: RDD[(String, Iterable[UserVisitAction])] = actionRDD.groupBy(_.session_id)
//5.2 将分组后的数据根据时间进行排序（升序）
val page2pageRDD: RDD[(String, List[String])] = sessionGroupRDD.mapValues(
  datas =&gt; {
    //1 将迭代器转成list,然后按照行动时间升序排序
    val actions: List[UserVisitAction] = datas.toList.sortBy(_.action_time)
    //2 转变数据结构,获取到pageId
    val pageidList: List[String] = actions.map(_.page_id)
    //3 根据排好序的页面List拉链获得单跳元组 (1,2,3,4) zip (2,3,4)
    val pageToPageList: List[(String, String)] = pageidList.zip(pageidList.tail)
    //4 再次转变结构 元组变字符串 (1,2) =&gt; 1-2
    val pageJumpCounts: List[String] = pageToPageList.map {
      case (p1, p2) =&gt; p1 + &quot;-&quot; + p2
    }
    //5 对分子也进行过滤下,提升效率 只保留1-2,2-3,3-4,4-5,5-6,6-7的数据
    pageJumpCounts.filter(
      str =&gt; zipIds.contains(str)
    )
  }
)

val pageFlowRDD: RDD[List[String]] = page2pageRDD.map(_._2)

//5.3 聚合统计结果
val reduceFlowRDD: RDD[(String, Int)] = pageFlowRDD.flatMap(list =&gt; list).map((_, 1)).reduceByKey(_ + _)

//6 分母和分组全部搞定,计算页面单跳转换率
reduceFlowRDD.foreach{
  case (pageflow,sum) =&gt; {
    val pageIds: Array[String] = pageflow.split(&quot;-&quot;)

    //根据pageIds(0) 取分母的值
    val pageIdSum: Int = idsMap.getOrElse(pageIds(0), 1)
    //页面单跳转换率 = 分子/分母
    println(pageflow + &quot; = &quot; + sum.toDouble / pageIdSum)
  }
}

//TODO 3 关闭资源
sc.stop()
</code></pre>
<p>}<br>
}</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[面试总结]]></title>
        <id>https://MoSpade.github.io/post/mian-shi-zong-jie/</id>
        <link href="https://MoSpade.github.io/post/mian-shi-zong-jie/">
        </link>
        <updated>2021-09-26T02:46:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="210108面试总结">210108面试总结</h1>
<h2 id="flume">Flume</h2>
<pre><code class="language-sql">--taildir source监控是目录还是文件--

--讲讲flume的事务，at least once的提交机制--
</code></pre>
<h2 id="kafka">Kafka</h2>
<pre><code class="language-sql">--Kafka挂了怎么确保精准一次性--

--Kafka遇到了什么问题--

--kafka zk连接broker信息，看有哪些topic中数据是什么样子用什么命令--

--看多少topic的命令--

--kafka有没有遇到数据重复问题--

--实时和离线一起运行的情况下，Kafka吞吐量多少？--

--kafka 遇到的问题 扩容问题怎么解决--
</code></pre>
<h2 id="hbase">HBase</h2>
<pre><code class="language-sql">--HBase是怎么处理热点问题的--

--HBase的scan和get区别--

--HBase二级索引--

--Phoenix写入时二级索引会生效，直接写入HBase呢，二级索引--

--在hBase中打散的劣势--


</code></pre>
<h2 id="hive">Hive</h2>
<pre><code class="language-sql">--Hive和MySQL区别--
    数据存储位置
    数据更新
    执行延迟
    数据规模
    不支持事务，也不支持索引
    语法上有开窗，炸裂侧写，支持多引擎，可以自定义UDF、UDTF，可以解析json，有自己的复杂数据结构
    列式存储
    分区分桶四个by
    
--Hive的分区分桶，rank、row_number,order by,sort by--

--impala和hive计算引擎区别--

--mr过程哪一步最消耗性能--

--Hive数据倾斜解决--

--hive三行转三列--

--列存的优点--

--Hive怎么实现更新一条数据--

--spark on hive 和 hive on spark--

--hive on spark对比sparksql--

--hive怎么优化--

--Hive 是如何把sql转化为mr程序. 比如两个表join 做了哪些操作,map 和reduce得输入输出分别是什么?--

--sqlAPi了解多少--

--order by  sort by distribute by 的区别--
</code></pre>
<pre><code class="language-sql">--sql:
--旅馆入住，求入住人和同住人，同住人的条件是同一旅店同一房间的人的入住时间或者离开时间在15分钟以内的--

--7天连续三天--

--要求不能开窗，编写一个 SQL 查询，查找所有至少连续出现三次的数字。--
</code></pre>
<h2 id="azkaban">Azkaban</h2>
<pre><code class="language-sql">--azakaban使用遇到的问题--
--Azkaban节点间依赖--
    mysql创建一个授权帐号
    编写Azkaban工作流程配置文件
    azkaban.project
    xxx.flow文件
    注意里面的dependsOn就是节点依赖
    多Executor
    {
        nodes:
      - name: mysql_to_hdfs
        type: command
        config:
         command: /home/atguigu/bin/mysql_to_hdfs.sh all ${dt}

      - name: hdfs_to_ods_log
        type: command
        config:
         command: /home/atguigu/bin/hdfs_to_ods_log.sh ${dt}

      - name: hdfs_to_ods_db
        type: command
        dependsOn: 
         - mysql_to_hdfs
        config: 
         command: /home/atguigu/bin/hdfs_to_ods_db.sh all ${dt}
        ......
    }
</code></pre>
<h2 id="sqoop">Sqoop</h2>
<pre><code class="language-sql">--sqoop导入导出的格式内容--
</code></pre>
<h2 id="redis">Redis</h2>
<pre><code class="language-sql">--Redis有几种类型--
    string
    hash
    list
    set
    sorted set
    （切忌不要说int）
    
--Redis搭建的是什么形式？
    单节点
    主从（哨兵模式）
    集群
    
--redis 遇到的雪崩 缓存 热点key 怎么解决--
</code></pre>
<h2 id="clickhouse">ClickHouse</h2>
<pre><code class="language-sql">--ClickHouse支不支持高并发--

--为什么用ClickHouse，ClickHouse和ES对比--

--你们实时用的ClickHouse数据量有多大？--

</code></pre>
<h2 id="elasticsearch">ElasticSearch</h2>
<pre><code class="language-sql">--ES版本号、命令--

</code></pre>
<h2 id="离线数仓">离线数仓</h2>
<pre><code class="language-sql">--离线使用什么调度，具体配置过程--

--为什么用星型模型，为什么不用雪花模型--
    星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余。
    当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。
    数据存储方面：星型架构有数据冗余，反范式化，雪花架构没有数据冗余。
    雪花模型在ETL操作，加载数据集市的数据是会比较复杂，因为维度表之间还存在关联模型，星型就相对简单很多。
    雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低，而星型模型的性能就会比较高。
    雪花模型更加适合维度分析的场景，星型模型更加适合指标分析的场景。
    
--雪花模型对比星型模型优势--
    在维度建模的基础上又分为三种模型：星型模型、星座模型和雪花模型
    雪花模型与星型模型的区别主要在于维度的层级，标准的星型模型维度只有一层，而雪花模型可能会涉及多级
    雪花模型，比较靠近3NF，但是无法完全遵守，因为遵循3NF的性能成本太高
    星座模型与前两种情况的区别是事实表的数量，星座模型是基于多个事实表
    基本上很多数据仓库的常态，因为很多数据仓库都是多个事实表的。所以星座不星星座只反映是否有多个事实表，他们之间是否共享一些维度表
    所以星座模型并不和前两个模型冲突
    首先就是星座不星座这个只跟数据和需求有关系，跟设计没关系，不用选择
    星型还是雪花，取决于性能优先，还是灵活更优先。
    目前企业开发中，不会绝对选择一种，根据情况灵活组合，甚至并存（一层维度和多层维度都保存）。但是整体来看，更货币于维度更少的星型模型。尤其是Hadoop休系，减少Join就是减少Shuffle，性能差距很大。（关系型数据可以依靠强大的主键索引）
    
--数仓建模举例说明一个ADS层指标是怎么计算出来的--

--数仓和数据库的区别--

--ods存hdfs，那么hive表是内部表还是外部表。--

--建离线模型，用内部还是外部表。--

--表在hdfs上存储格式--

--压缩方法有哪几种--

--数仓dwd做了哪些处理。--

--指标倒推dwd的问题，新加了指标dwd粒度没有，那又要重构怎么办--

--改指标改dwd表怎么办，面向政府客户要我们不停改怎么办--

--事实表是如何确定的--

--事实表有哪些表，订单表有哪些维度，有没有用过总线矩阵--

--几亿级数据量平时需要几百万数据量的计算如何使用OLAP实现？--

--介绍下数仓分层, 你们怎么分的, 以及每一层得作用--

</code></pre>
<h2 id="实时数仓sparkflink">实时数仓（Spark&amp;&amp;Flink）</h2>
<pre><code class="language-sql">--Flink和SparkStreaming区别--
    Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。
    #主要角色的不同
    Spark：Master/Worker/Driver/Executor
    Flink:JobManager/TaskManager/Slot
    #任务调度的不同
    SparkStreaming连续不断的生成微小的数据批次，构建有向无环图DAG，依次创建DStreamGraph/JobGenerator/JobScheduler
    Flink根据用户提交的代码生成StreamGraph，经过优化生成JobGraph，然后提交给JobManager进行处理，JobManager会根据JobGraph生成ExecutionGraph,ExecutionGraph是Flink调度最核心的数据结构，JobManager根据ExecutionGraph对Job进行调度
    #时间机制的不同
    SparkStreaming只支持处理时间
    Flink支持：处理时间、事件时间、注入时间，同时也支持WaterMark机制来处理滞后数据
    #容错机制的不同
    SparkStreaming，可以设置checkpoint，然后假如发生故障并重启，我们可以从上次checkpoint之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次语义
    Flink则使用两阶段提交协议来解决这个问题。
    
--提交流程、算子底层、底层通信、容错机制、有几种backend，基于memory的存在哪里--

--去重操作在哪一层--

--SpringBoot和展示的对接--

--groupByKey和aggregateByKey区别--

--SparkStreaming有什么--

--SparkStreaming有状态和无状态--

--SparkStreaming持久化--

--你们实时用的ClikcHouse数据量有多大？--

--讲一下flink提交到yarn得流程--

--flink状态有哪些--

--哪些业务场景用到了状态: 广播 去重 聚合等操作--

--interval join得缺陷--
	不能处理超时数据
	
--讲一下flink使用过程中的问题 有没有集群宕机的情况:--

--讲一下waterMark和三种时间语义--

--讲一下ck savepoint--

--讲一下barrier  对齐的操作, 非对齐会产生什么情况--

--flink遇到的问题, flink消费能力不足的调优--

--介绍实时项目的pipeline--

--介绍下数仓分层, 你们怎么分的, 以及每一层得作用--

--对实时数据做一个topN的,怎么做: 使用ListState--

--flink算子的一些劣势--

--flinkcheckPoint 怎么实现 源码层--

--一条数据从windowAssigner进入 到 trigger 窗口计算做了哪些事情, 结合waterMark机制讲--

--对一个非常大的流数据进行排序,怎么做--

--介绍flinkwatermark 和watermark的传递机制。--

--介绍flink消费kafka主题并行度问题--

--flink提交用到了哪些参数 申请资源的有哪些--

--taskmanager是什么 slot是什么--

--slot的划分机制--

--flink求topn，手写代码，ds怎么做--

--flink rpc通信机制--

--flink序列化机制--

--可视化，预警（预警了什么样的信息）--

--元数据管理相关（keys-by-keys的指标校验）--

</code></pre>
<h2 id="线程">线程</h2>
<pre><code class="language-sql">--start()和run()方法有什么区别--
    用start方法来启动线程，真正实现了多线程运行，这时无需等待run方法体代码执行完毕而直接继续执行下面的代码。通过调用Thread类的start()方法来启动一个线程，这时此线程处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行run()方法，这里方法run()称为线程体，它包含了要执行的这个线程的内容，run方法运行结束，此线程随即终止。
    run()方法只是类的一个普通方法而已，如果直接调用Run方法，程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到写线程的目的。
    
--如何用多线程自己实现一个高并发且线程安全的Map--

</code></pre>
<h2 id="个人">个人</h2>
<pre><code class="language-sql">--大数据擅长、弱势的地方的地方--

--提供数据给谁看，哪些人关注这个数据？--

--用过哪些数据库--

--具体工作职责--

--用shell写过什么脚本--

</code></pre>
]]></content>
    </entry>
</feed>